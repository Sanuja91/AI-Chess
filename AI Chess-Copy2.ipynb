{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPMIND ALPHAZERO CHESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- DONE!! Experiences are not stored properly since ZH is sometimes not avaiable in MCTS storage\n",
    "- DONE!! DOUBLE CHECK!!! Handle the hidden state properly as the AI is navigating the tree! \n",
    "\n",
    "\n",
    "## TODO LATER\n",
    "#### - Get backward possibly loss to train the controller. Might need to save the tensor output like states, values, etc in MCTS\n",
    "- Create Self play class\n",
    "- Create Arena class\n",
    "\n",
    "DEEP MIND OPEN ACCESS PAPER \n",
    "\n",
    "https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd\n",
    "\n",
    "RAY ALPHA ZERO IMPLEMENTATION \n",
    "\n",
    "https://github.com/ray-project/ray/tree/master/rllib/contrib/alpha_zero\n",
    "\n",
    "DUPLICATED MCTS IMPLEMENTATION\n",
    "\n",
    "https://github.com/suragnair/alpha-zero-general/blob/master/MCTS.py\n",
    "\n",
    "DISTRIBUTED IMPLEMENTATION\n",
    "\n",
    "https://github.com/mokemokechicken/reversi-alpha-zero/blob/master/src/reversi_zero/lib/ggf.py\n",
    "\n",
    "CHESS MOVES\n",
    "\n",
    "https://www.ichess.net/blog/chess-pieces-moves/\n",
    "\n",
    "BOARD REPRESENTATIONS\n",
    "\n",
    "https://medium.com/datadriveninvestor/reconstructing-chess-positions-f195fd5944e\n",
    "\n",
    "ALPHA ZERO EXPLANATION\n",
    "\n",
    "https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/\n",
    "\n",
    "TRANSFORMER NETWORK IMPLEMENTATION\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user python-chess gym colorama tqdm PyQt5 ray tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess, gym, pickle, random, torch, math\n",
    "import chess.svg as svg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from copy import copy, deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "# plt.style.use('ggplot')\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from copy import deepcopy\n",
    "from colorama import init, Fore, Back, Style\n",
    "from collections import Counter, deque\n",
    "from tqdm import tqdm, tnrange, notebook\n",
    "from random import shuffle\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from PyQt5.QtSvg import QSvgWidget\n",
    "from PyQt5.QtWidgets import QApplication, QWidget\n",
    "\n",
    "from utilities import *\n",
    "# import constants \n",
    "from constants import *\n",
    "from models.vae import CNN_AE, CNN_VAE, train_ae, train_vae, Conv\n",
    "from models.mdn import MDN_RNN, loss_function, clip_grad_norm_\n",
    "from models.controller import Controller\n",
    "\n",
    "# init(convert = True)\n",
    "# REFERENCES\n",
    "# Tensorflow implementation for Chess\n",
    "# https://github.com/saurabhk7/chess-alpha-zero\n",
    "\n",
    "# Pytorch implementation for Connect4\n",
    "# https://github.com/plkmo/AlphaZero_Connect4/tree/master/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chess_Environment(gym.Env):\n",
    "    \"\"\"Chess Environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.white_pieces = ['P', 'N', 'B', 'R', 'Q', 'K']\n",
    "        self.black_pieces = [piece.lower() for piece in self.white_pieces]\n",
    "        self.x_coords, self.y_coords = np.meshgrid(list(range(0, 8)), list(range(0, 8)))\n",
    "        self.x_coords = self.x_coords / 7\n",
    "        self.y_coords = self.y_coords / 7\n",
    "        self.state_size = self.observe()[0].shape\n",
    "        self.init_action_decoder()\n",
    "        self.whites_turn = True\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment\"\"\"\n",
    "        self.board = chess.Board()\n",
    "        \n",
    "    def terminal_test(self):\n",
    "        \"\"\"Checks if the game is over\"\"\"\n",
    "        return self.board.is_game_over(claim_draw = True)\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\"Gives the end game result\"\"\"\n",
    "\n",
    "        result = self.board.result(claim_draw = True)\n",
    "        if result == '1-0':\n",
    "            return 1\n",
    "        elif result == '0-1':\n",
    "            return -1\n",
    "        elif result == '1/2-1/2':\n",
    "            return 0.1\n",
    "        elif result == '*':\n",
    "            return 0\n",
    "        else:\n",
    "            raise Exception('Invalid Result', result)\n",
    "        \n",
    "    def legal_actions(self):\n",
    "        \"\"\"Provides a list of legal actions in current state\"\"\"\n",
    "        legal_actions = [str(legal_action) for legal_action in list(self.board.legal_moves)]\n",
    "        return legal_actions\n",
    "    \n",
    "    def encode(self):\n",
    "        \"\"\"Encodes game state into a string\"\"\"\n",
    "        board_ = self.board.piece_map()\n",
    "            \n",
    "        encoded = {\n",
    "            'board' : board_,\n",
    "            'turn' : self.board.turn,\n",
    "            'legal_actions' : self.legal_actions()\n",
    "        }\n",
    "        \n",
    "        return pickle.dumps(encoded)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Decodes string into game state and sets board and turn\"\"\"\n",
    "        decoded = pickle.loads(encoded)\n",
    "        self.board.set_piece_map(decoded['board'])\n",
    "        if decoded['turn'] == False:\n",
    "            self.board = self.board.mirror()\n",
    "        self.board.turn = decoded['turn']\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"Create observation from the game state\"\"\"\n",
    "\n",
    "        board_ = copy(self.board)\n",
    "            \n",
    "        board_ = np.ndarray.flatten(np.array(board_.__str__().split())).reshape(8, 8)\n",
    "        \n",
    "        black_pawns = np.isin(copy(board_), ['p']).astype(int)\n",
    "        black_knights = np.isin(copy(board_), ['n']).astype(int)\n",
    "        black_rooks = np.isin(copy(board_), ['r']).astype(int)\n",
    "        black_bishops = np.isin(copy(board_), ['b']).astype(int)\n",
    "        black_queen = np.isin(copy(board_), ['q']).astype(int)\n",
    "        black_king = np.isin(copy(board_), ['k']).astype(int)\n",
    "         \n",
    "        white_pawns = np.isin(copy(board_), ['P']).astype(int)\n",
    "        white_knights = np.isin(copy(board_), ['N']).astype(int)\n",
    "        white_rooks = np.isin(copy(board_), ['R']).astype(int)\n",
    "        white_bishops = np.isin(copy(board_), ['B']).astype(int)\n",
    "        white_queen = np.isin(copy(board_), ['Q']).astype(int)\n",
    "        white_king = np.isin(copy(board_), ['K']).astype(int)\n",
    "        \n",
    "        state = np.array([\n",
    "            white_pawns,\n",
    "            white_knights,\n",
    "            white_rooks,\n",
    "            white_bishops,\n",
    "            white_queen,\n",
    "            white_king,\n",
    "            black_pawns,\n",
    "            black_knights,\n",
    "            black_rooks,\n",
    "            black_bishops,\n",
    "            black_queen,\n",
    "            black_king\n",
    "        ])\n",
    "        \n",
    "        return state, self.legal_actions()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Perform a step in the environment\"\"\"\n",
    "        try:\n",
    "            self.board.push_uci(action)\n",
    "        except ValueError:\n",
    "            print(f\"INVALID ACTION {action}.\\nSELECTING A RANDOM ACTION FROM LEGAL ACTIONS\\n{self.legal_actions()}\")\n",
    "            actions = random.choices(self.legal_actions())\n",
    "            self.board.push_uci(actions[0])\n",
    "            \n",
    "        self.board = self.board.mirror()\n",
    "        if self.whites_turn:\n",
    "            self.whites_turn = False\n",
    "        else:\n",
    "            self.whites_turn = True\n",
    "        return self.observe()\n",
    "    \n",
    "    def move_board(self, move):\n",
    "        \"\"\"Moves the board positions as per the move\"\"\"\n",
    "\n",
    "        char_to_int = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8} \n",
    "\n",
    "        int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "        encoded_board = [(char_to_int[pos[0]], int(pos[1])) for pos in np.ndarray.flatten(BOARD)]\n",
    "\n",
    "        new_board = [tuple(map(sum, zip((char_to_int[pos[0]], int(pos[1])), move))) for pos in np.ndarray.flatten(BOARD)]\n",
    "        moves = []\n",
    "        for pos, new_pos in zip(np.ndarray.flatten(BOARD), new_board):\n",
    "            try:\n",
    "                if new_pos[1] > 8:\n",
    "                    raise Exception()\n",
    "                if move[2] is None:\n",
    "                    move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}'\n",
    "                else:\n",
    "                    move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}{move[2]}'\n",
    "            \n",
    "                if '-' in move_:\n",
    "                    raise Exception()\n",
    "            \n",
    "                if '0' in move_:\n",
    "                    raise Exception()\n",
    "                \n",
    "            except Exception:\n",
    "                move_ = 'XXXX'\n",
    "            moves.append(move_)   \n",
    "        return np.array(moves).reshape(8, 8)\n",
    "    \n",
    "    def init_action_decoder(self):\n",
    "        \"\"\"Initialize the decoder to decode the actions\"\"\"\n",
    "        decoder = []\n",
    "        for key in MOVES.keys():\n",
    "            decoder_ = self.move_board(MOVES[key])\n",
    "            decoder.append(decoder_)\n",
    "#             print(f'Move {key}\\n', new_board, '\\n')\n",
    "    \n",
    "        self.decoder = np.array(decoder)\n",
    "        self.action_size = self.decoder.shape\n",
    "        \n",
    "    def select_action(self, logits):\n",
    "        \"\"\"Decodes the output from the NN to legal actions\"\"\" \n",
    "        decoder_ = np.ndarray.flatten(self.decoder)\n",
    "        logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "        move_logits = [(decoder_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "        move_logits = dict(move_logits)\n",
    "        \n",
    "        legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in self.legal_actions()}\n",
    "        probabilities = list(legal_move_logits.values()) / sum(list(legal_move_logits.values()))\n",
    "\n",
    "        action = random.choices(list(legal_move_logits.keys()), weights = probabilities, k = 1)[0]\n",
    "        return action\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render chess board\"\"\"\n",
    "        chess.svg.board(board = self.board)  \n",
    "\n",
    "env = Chess_Environment()\n",
    "env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, decoder, action_size, vae, mdn, controller, test_mode = False, batch_size = 64, lr = 1e-4, legal_multiplier = 2, save_freq = 2):\n",
    "        self.batch_size = batch_size\n",
    "        self.action_size = action_size\n",
    "        self.legal_multiplier = legal_multiplier # To increase the value of legal actions and MCTS insight as the target policy\n",
    "        self.decoder = decoder\n",
    "        self.save_freq = save_freq\n",
    "        if vae is not None:\n",
    "            self.vae = CNN_AE(vae, None, 'Latest')\n",
    "            self.vae.eval()\n",
    "        if mdn is not None:\n",
    "            self.mdn = MDN_RNN(mdn, None, 'Latest')\n",
    "            params = {\n",
    "                'hidden_size' : self.mdn.hidden_size,\n",
    "                'z_size' : self.vae.z_size,\n",
    "                'action_size' : action_size,\n",
    "                'expansion_size': 1200\n",
    "            }\n",
    "            self.mdn.eval()\n",
    "            if test_mode:\n",
    "                self.controller = Controller('Test', params, 'Latest')\n",
    "            else:\n",
    "                self.controller = Controller('Test', params, False)\n",
    "                self.controller.save_model(0)\n",
    "        \n",
    "            self.optimizer = Adam(self.controller.parameters(), lr = lr)\n",
    "        \n",
    "        self.char_to_int = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8} \n",
    "        self.char_to_int_promo = {'r': 1, 'b': 2, 'q': 3, 'k': 4, 'n': 5, '': 6} \n",
    "        \n",
    "        moves_df = pd.DataFrame(list(zip(self.char_to_int.values(), self.char_to_int.keys())), columns = ['Ids', 'Labels'])\n",
    "        promo_df = pd.DataFrame(list(zip(self.char_to_int_promo.values(), self.char_to_int_promo.keys())), columns = ['Ids', 'Labels'])\n",
    "        self.move_actions_ohe = pd.get_dummies(moves_df['Ids'])\n",
    "        self.promo_actions_ohe = pd.get_dummies(promo_df['Ids'])\n",
    "        \n",
    "        self.criterion_pi = nn.NLLLoss()\n",
    "        self.criterion_v = nn.MSELoss()\n",
    "        self.trained_times = 0\n",
    "    \n",
    "    def select_action(self, logits, legal_actions):\n",
    "        \"\"\"Decodes the output from the NN to legal actions\"\"\" \n",
    "        decoder_ = np.ndarray.flatten(self.decoder)\n",
    "        logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "        move_logits = [(decoder_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "        move_logits = dict(move_logits)\n",
    "        \n",
    "        legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in legal_actions}\n",
    "        probabilities = list(legal_move_logits.values()) / sum(list(legal_move_logits.values()))\n",
    "        \n",
    "        legal_actions = list(legal_move_logits.keys())\n",
    "        action = random.choices(list(legal_move_logits.keys()), weights = probabilities, k = 1)[0]\n",
    "        \n",
    "        action_probabilities = {legal_actions[idx] : probabilities[idx] for idx in range(len(legal_actions))}\n",
    "        return action, action_probabilities\n",
    "    \n",
    "    def one_hot_encode_action(self, action):\n",
    "        \"\"\"One hot encodes the action\"\"\"\n",
    "        if len(action) == 4:\n",
    "            encoded_action = [\n",
    "                self.move_actions_ohe.loc[\n",
    "                self.char_to_int[action[0]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[1]) - 1].values, \n",
    "                self.move_actions_ohe.loc[self.char_to_int[action[2]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[3]) - 1].values,\n",
    "                self.promo_actions_ohe.loc[self.char_to_int_promo[''] - 1].values\n",
    "            ]\n",
    "        else:\n",
    "            encoded_action = [\n",
    "                self.move_actions_ohe.loc[\n",
    "                self.char_to_int[action[0]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[1]) - 1].values, \n",
    "                self.move_actions_ohe.loc[self.char_to_int[action[2]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[3]) - 1].values,\n",
    "                self.promo_actions_ohe.loc[self.char_to_int_promo[action[4]] - 1].values\n",
    "            ]\n",
    "        ohe_action = np.concatenate(encoded_action)\n",
    "        return ohe_action\n",
    "    \n",
    "    def reset(self, batch_size):\n",
    "        \"\"\"Resets the MDNs hidden state\"\"\"\n",
    "        self.hidden = self.mdn.init_hidden(batch_size)\n",
    "        self.hidden = (self.hidden[0].detach().to('cpu'), self.hidden[1].detach().to('cpu'))\n",
    "        \n",
    "    def act(self, state, legal_actions):\n",
    "        \"\"\"Gets an action from the agent\"\"\"\n",
    "        state = torch.tensor(state).float().unsqueeze(0)\n",
    "        z = self.vae.encode(state)\n",
    "    \n",
    "        zh = torch.cat((z.to('cpu'), self.hidden[0].squeeze(0).to('cpu')), dim = 1)\n",
    "        if zh.shape[0] > 1:\n",
    "            raise Exception('Batch size > 1 not handled')\n",
    "            \n",
    "        logits, values = self.controller(zh)\n",
    "        logits, value = logits.detach().squeeze(0).cpu().numpy(), values.detach().squeeze(0).cpu().numpy()[0]\n",
    "        action, action_probabilities = self.select_action(logits, legal_actions)\n",
    "        \n",
    "        return action_probabilities, value, zh\n",
    "    \n",
    "    def encode_zh(self, state):\n",
    "        \"\"\"Encodes the state with the hidden state\"\"\"\n",
    "        state = torch.tensor(state).float().unsqueeze(0)\n",
    "        z = self.vae.encode(state)\n",
    "        zh = torch.cat((z.to('cpu'), self.hidden[0].squeeze(0).to('cpu')), dim = 1)\n",
    "        return zh\n",
    "    \n",
    "    def train(self, experiences):\n",
    "        \"\"\"Trains the controller from the MCTS experiences\"\"\"\n",
    "        batches = [experiences[i : i + self.batch_size] for i in range(0, len(experiences), self.batch_size)]\n",
    "        \n",
    "        for batch in deepcopy(batches):\n",
    "            zh, target_action_probs, returns = zip(*batch)\n",
    "            returns = torch.tensor(returns).float()\n",
    "            zh = torch.tensor(np.concatenate(zh, axis = 0)).float()\n",
    "            self.optimizer.zero_grad()\n",
    "            logits, values = self.controller(zh)\n",
    "            target_policies = self.target_policies(logits, target_action_probs)\n",
    "            \n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            \n",
    "            for idx in range(logits.shape[0]):\n",
    "                advantage = returns[idx] - values[idx]\n",
    "                # converts logits to action probabilities\n",
    "#                 print(values[idx].squeeze(0).shape, returns[idx].shape)\n",
    "#                 print(values[idx].squeeze(0), returns[idx])\n",
    "                policy_losses.append(-F.softmax(logits[idx], dim = 0) * advantage)\n",
    "                value_losses.append(F.smooth_l1_loss(values[idx].squeeze(0), returns[idx]))\n",
    "            \n",
    "            # sum up all the values of policy_losses and value_losses\n",
    "            self.controller.train_steps += 1\n",
    "            policy_loss = torch.stack(policy_losses).mean()\n",
    "            value_loss = torch.stack(value_losses).mean()\n",
    "            writer.add_scalar('policy_loss', policy_loss, self.controller.train_steps)\n",
    "            writer.add_scalar('value_loss', value_loss, self.controller.train_steps)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.trained_times % self.save_freq == 0:\n",
    "            print(\"SAVING\")\n",
    "            self.controller.save_model(self.trained_times)\n",
    "        self.trained_times += 1\n",
    "        \n",
    "    def load_hidden(self, hidden):\n",
    "        \"\"\"Loads the hidden state to the MDN RNN\"\"\"\n",
    "        self.hidden = deepcopy(hidden)\n",
    "        \n",
    "    def get_hidden(self):\n",
    "        \"\"\"Gets the hidden state from the MDN RNN\"\"\"\n",
    "        return deepcopy(self.hidden)\n",
    "    \n",
    "    def random_controller(self): \n",
    "        \"\"\"Updates the controller from a randomly selected save file\"\"\"\n",
    "        self.controller.load_model('Random')\n",
    "        \n",
    "    def update_hidden(self, state, action):\n",
    "        \"\"\"Updates the hidden state of the MDN-RNN\"\"\"\n",
    "        state = torch.tensor(state).float().unsqueeze(0)\n",
    "        mu, logvar = self.vae.encode(state)\n",
    "        z = self.vae.reparameterize(mu, logvar).squeeze(-1)\n",
    "        ohe_action = self.one_hot_encode_action(action)\n",
    "        za = torch.cat((z.to('cpu'), torch.tensor(ohe_action).float().to('cpu').unsqueeze(0)), dim = 1)\n",
    "        _, self.hidden = self.mdn(za.unsqueeze(1), self.hidden)\n",
    "        self.hidden = (self.hidden[0].detach().to('cpu'), self.hidden[1].detach().to('cpu'))\n",
    "        \n",
    "                \n",
    "    def target_policies(self, logits, legal_move_logits):\n",
    "        \"\"\"Takes the logits from the current policy and layers the insights from MCTS\"\"\"\n",
    "        new_boards__ = [[x.lower() for x in list(np.ndarray.flatten(self.decoder))] for _ in range(logits.shape[0])]\n",
    "        target_policies = []\n",
    "        for idx in range(logits.shape[0]):\n",
    "            new_boards___ = new_boards__[idx]\n",
    "            logits__ = logits[idx].view(-1)\n",
    "            legal_move_logits__ = {k.lower(): v * self.legal_multiplier for k, v in legal_move_logits[idx].items()}\n",
    "            for move, legal_move_logit in legal_move_logits__.items():\n",
    "                index = new_boards___.index(move)\n",
    "                logits__[index] += legal_move_logit\n",
    "            target_policies.append(logits__)\n",
    "        target_policies = torch.cat(target_policies, dim = 0).reshape(logits.shape[0], 76, 8, 8).float()\n",
    "        return target_policies\n",
    "    \n",
    "agent = Agent(env.decoder, env.action_size, 'Test', 'Test', 'Test', test_mode = False)\n",
    "agent.reset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = Agent(env.decoder, env.action_size, 'Test', 'Test', 'Test', test_mode = False)\n",
    "\n",
    "# agent.reset(1)\n",
    "\n",
    "# class MCTS():\n",
    "#     \"\"\"Monte Carlo Tree Search Algorithm geared for Neural Networks\"\"\"\n",
    "\n",
    "#     def __init__(self, env, agent, mcts_simulations = 100, max_depth = 100, delta = 0.5):\n",
    "#         self.env = env\n",
    "#         self.agent = agent\n",
    "#         self.cpuct = 0.2    # WARNING BULLSHIT NUMBER!\n",
    "#         self.delta = delta  # value to prevent crash if no edges are visited\n",
    "#         self.mcts_simulations = mcts_simulations\n",
    "#         self.max_depth = max_depth\n",
    "        \n",
    "#         self.Qsa = {}       # stores Q values for s, a (as defined in the paper)\n",
    "#         self.Nsa = {}       # stores # times edge s, a was visited\n",
    "#         self.Ns = {}        # stores # times board s was visited\n",
    "#         self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "#         self.Es = {}        # stores victory result (1, 0, -1) ended for board s\n",
    "#         self.Vs = {}        # stores legal actions for board s\n",
    "#         self.zh = {}        # stores preserved state (s) that is fed into the controller\n",
    "#         self.hidden = {}    # stores the hidden state of the MDN-RNN of every (s,a) pair\n",
    "\n",
    "#     def action_probabilities(self, encoded_state, temp = 1):\n",
    "#         \"\"\"\n",
    "#         This function performs numMCTSSims simulations of MCTS starting from\n",
    "#         canonicalBoard.\n",
    "#         Returns:\n",
    "#             probs: a policy vector where the probability of the ith action is\n",
    "#                    proportional to Nsa[(s,a)]**(1./temp)\n",
    "#         \"\"\"\n",
    "#         for runs in range(self.mcts_simulations):\n",
    "# #             print(f\"{Fore.BLUE}MCTS SIMULATION {runs + 1}{Style.RESET_ALL}\")\n",
    "#             self.search(encoded_state, 0)\n",
    "        \n",
    "#         s = encoded_state\n",
    "#         self.env.decode(encoded_state)\n",
    "#         legal_actions = self.env.legal_actions()\n",
    "#         counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in legal_actions]\n",
    "#         # counts array represent the number of time each action edge from your current state was traversed\n",
    "\n",
    "#         if temp == 0: # temprature is 0 representing taking the best action possible (greedy)\n",
    "#             bestA = np.argmax(counts) # bestA: best action number : argmax Returns the indices of the maximum values\n",
    "#             probs = [0] * len(counts)\n",
    "#             probs[bestA] = 1\n",
    "#             action_probs = {legal_actions[idx] : probs[idx] for idx in range(len(legal_actions))}\n",
    "#             return action_probs # returns the definite move(s) with same greedy reward, out of which one move HAS to be played\n",
    "        \n",
    "# #         print(f\"{Fore.BLUE}COUNTS {sum(counts)} {counts}{Style.RESET_ALL}\")\n",
    "        \n",
    "#         # Handles frequent draw situation when MCTS fails to explore when the game is over resulting 0 counts causing div 0 error\n",
    "#         if sum(counts) == 0:\n",
    "#             counts = [1 for _ in counts]\n",
    "#         counts = [x ** (1. / temp) + 0.5 for x in counts]\n",
    "#         probs = [x / float(sum(counts)) for x in counts]\n",
    "#         action_probs = {legal_actions[idx] : probs[idx] for idx in range(len(legal_actions))}\n",
    "#         # If the game isn't over return the state zh or return None at the end\n",
    "# #         if self.env.result() != 0:\n",
    "# #             return action_probs, None\n",
    "#         return action_probs, self.zh[s]  \n",
    "#         #returns the probablity of different moves that CAN be played resulting in uniform distribution\n",
    "\n",
    "\n",
    "#     def search(self, encoded_state, depth):\n",
    "#         \"\"\"\n",
    "#         This function performs one iteration of MCTS. It is recursively called\n",
    "#         till a leaf node is found. The action chosen at each node is one that\n",
    "#         has the maximum upper confidence bound as in the paper.\n",
    "#         Once a leaf node is found, the neural network is called to return an\n",
    "#         initial policy P and a value v for the state. This value is propogated\n",
    "#         up the search path. In case the leaf node is a terminal state, the\n",
    "#         outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "#         updated.\n",
    "#         NOTE: the return values are the negative of the value of the current\n",
    "#         state. This is done since v is in [-1,1] and if v is the value of a\n",
    "#         state for the current player, then its value is -v for the other player.\n",
    "#         Returns:\n",
    "#             v: the negative of the value of the current canonicalBoard\n",
    "#         \"\"\"\n",
    "\n",
    "#         s = deepcopy(encoded_state)\n",
    "        \n",
    "#         # loads the appropriate hidden state for MDN-RNN else saves it\n",
    "#         if s not in self.hidden:\n",
    "#             self.hidden[s] = self.agent.get_hidden()\n",
    "#         else:\n",
    "#             self.agent.load_hidden(self.hidden[s])\n",
    "        \n",
    "#         self.env.decode(encoded_state)\n",
    "# #         print(f\"{Fore.GREEN}SEARCH DEPTH {depth}{Style.RESET_ALL}\")\n",
    "        \n",
    "#         # Check if its an terminal state, -1 Opponent Won, 0 Game not Over, 1 Player Won\n",
    "#         if s not in self.Es:\n",
    "#             self.Es[s] = self.env.result()\n",
    "#         if self.Es[s] != 0 or self.env.terminal_test():\n",
    "#             state, _ = self.env.observe()\n",
    "#             self.zh[s] = self.agent.encode_zh(state) \n",
    "# #             print(\"SIMULATION OVER!\", self.env.terminal_test(), -self.Es[s], self.env.board.result())\n",
    "#             return -self.Es[s]\n",
    "        \n",
    "#         state, legal_actions = self.env.observe()\n",
    "# #         legal_actions = self.env.legal_actions()\n",
    "\n",
    "#         if s not in self.Ps: #if the current state 's' is not explored/expanded before n=0 by MCTS then create a new node and rollout\n",
    "#             self.Ps[s], v, self.zh[s] = self.agent.act(state, legal_actions)\n",
    "# #             print(\"self.Ps[s] at Depth\", depth, \"\\n\", self.Ps[s])\n",
    "#             valids = legal_actions\n",
    "\n",
    "#             self.Vs[s] = valids \n",
    "#             self.Ns[s] = 0\n",
    "# #             print(\"VALUE\", -v)\n",
    "#             return -v\n",
    "        \n",
    "#         valids = self.Vs[s] #as already visited the valid moves array 'Vs' is already initialized\n",
    "#         cur_best = -float('inf')\n",
    "#         best_act = -1\n",
    "        \n",
    "#         self.env.decode(encoded_state)\n",
    "#         legal_actions = self.env.legal_actions()\n",
    "#         # pick the action with the highest upper confidence bound\n",
    "        \n",
    "#         agent_actions = list(self.Ps[s].keys())\n",
    "        \n",
    "#         # Handles the occasional legal action that isn't an actual legal action\n",
    "#         # Pawn side cut even though there is no enemy piece in respective position\n",
    "# #         print(\"XXXXX PRE LEGAL ACTIONS\\n\", legal_actions)\n",
    "#         if not set(agent_actions) == set(legal_actions):\n",
    "#             legal_actions = agent_actions\n",
    "# #         print(\"XXXXX POST LEGAL ACTIONS\\n\", legal_actions)\n",
    "#         for a in legal_actions:\n",
    "#             if (s,a) in self.Qsa:\n",
    "#                 u = self.Qsa[(s,a)] + self.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (1 + self.Nsa[(s,a)])\n",
    "# #                 print(\"In Qsa\")\n",
    "#             else:\n",
    "#                 self.Ps[s][a] \n",
    "#                 u = self.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s])     # Q = 0 ? : node exists but not explored as added and initilized during nnet phase\n",
    "# #                 print(\"Not In Qsa\", self.Ps[s][a], self.Ns[s], math.sqrt(self.Ns[s]))\n",
    "# #             print(\"U\", u)\n",
    "#             if math.isnan(u):\n",
    "#                 u = 0\n",
    "# #                 print(\"Override U\", u)\n",
    "#             if u > cur_best:\n",
    "#                 cur_best = u\n",
    "#                 best_act = a\n",
    "# #         print(f\"{Fore.GREEN}BEST ACTION {best_act}{Style.RESET_ALL}\")\n",
    "#         a = best_act\n",
    "#         self.agent.update_hidden(state, a)\n",
    "        \n",
    "        \n",
    "#         _, _ = self.env.step(a)\n",
    "#         encoded_next_state = self.env.encode()\n",
    "\n",
    "#         v = self.search(encoded_next_state, depth + 1) # RECURSION until leaf node or terminal node is found\n",
    "        \n",
    "#         self.env.decode(encoded_state)\n",
    "        \n",
    "#         if (s,a) in self.Qsa:\n",
    "#             self.Qsa[(s,a)] = (self.Nsa[(s,a)] * self.Qsa[(s,a)] + v) / (self.Nsa[(s,a)] + 1) #update the Q Value\n",
    "#             self.Nsa[(s,a)] += 1 # increment number of visits to this node in MCTS\n",
    "#         else:\n",
    "#             self.Qsa[(s,a)] = v # initialize the new node\n",
    "#             self.Nsa[(s,a)] = 1\n",
    "\n",
    "#         self.Ns[s] += 1\n",
    "#         return -v\n",
    "    \n",
    "#     def reset(self):\n",
    "#         \"\"\"Resets the tracked information\"\"\"\n",
    "#         self.Qsa = {}       \n",
    "#         self.Nsa = {}       \n",
    "#         self.Ns = {}        \n",
    "#         self.Ps = {}       \n",
    "#         self.Es = {}\n",
    "#         self.Vs = {}  \n",
    "#         self.zh = {}\n",
    "#         self.agent.reset(1)\n",
    "#         self.env.reset()\n",
    "\n",
    "# for _ in range(5):\n",
    "#     env.reset()\n",
    "#     mcts_white = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = 20)\n",
    "#     mcts_black = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = 20)\n",
    "#     game_move = 0\n",
    "#     while not env.terminal_test():\n",
    "#     #     print(f\"{Fore.RED}GAME MOVE {game_move}{Style.RESET_ALL}\")\n",
    "#         encoded_state = env.encode()\n",
    "#         if env.whites_turn: \n",
    "#             action_probs, zh = mcts_white.action_probabilities(encoded_state)\n",
    "#         else:\n",
    "#             action_probs, zh = mcts_black.action_probabilities(encoded_state)\n",
    "        \n",
    "        \n",
    "# #         print(game_move, env.whites_turn, env.terminal_test(), type(zh))\n",
    "# #         if zh == None:\n",
    "# #             print(encoded_state)\n",
    "#         action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "# #         print(f\"{Fore.CYAN}ACTUAL MOVE\\n {action_probs}{Style.RESET_ALL}\")\n",
    "#         _, _ = env.step(action)\n",
    "    \n",
    "#         game_move += 1    \n",
    "#     print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! GAME COMPLETED !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, legal_actions = env.observe()\n",
    "\n",
    "# state_planes = [\n",
    "#     'White Pawns',\n",
    "#     'White Knights',\n",
    "#     'White Rooks',\n",
    "#     'White Bishops',\n",
    "#     'White Queen',\n",
    "#     'White King',\n",
    "#     'Black Pawns',\n",
    "#     'Black Knights',\n",
    "#     'Black Rooks',\n",
    "#     'Black Bishops',\n",
    "#     'Black Queen',\n",
    "#     'Black King'\n",
    "# ]\n",
    "\n",
    "# for inx, state_ in enumerate(state):\n",
    "#     fig = plt.figure()\n",
    "#     fig.suptitle(state_planes[inx], fontsize = 20)\n",
    "#     xticks = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "#     yticks = list(range(1, 9))\n",
    "#     yticks.reverse()\n",
    "#     plt.xticks(list(range(0, 8)), xticks)\n",
    "#     plt.yticks(list(range(0, 8)), yticks)\n",
    "#     plt.imshow(state_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queen_directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
    "# queen_distance = list(range(1, 8))\n",
    "# queen_moves = [f'{direction}{distance}' for direction in queen_directions for distance in queen_distance]\n",
    "\n",
    "# knight_moves = ['2N1E', '1N2E', '1S2E', '2S1E', '2S1W', '1S2W', '1N2W', '2N1W']\n",
    "# underpromotion_moves = ['DOUBLEM', 'NECUT', 'NWCUT']\n",
    "# moves = queen_moves + knight_moves + underpromotion_moves\n",
    "\n",
    "# print(f'{len(queen_moves)} QUEEN MOVES {queen_moves[: 15]}\\n{len(knight_moves)} KNIGHT MOVES {knight_moves}\\n{len(underpromotion_moves)} UNDERPROMOTION MOVES {underpromotion_moves}')\n",
    "# print(f'\\nTOTAL MOVES {len(moves)}')\n",
    "# print(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def move_board(move):\n",
    "#     \"\"\"Moves the board positions as per the move\"\"\"\n",
    "#     print(move)\n",
    "#     board = np.array([\n",
    "#         ['A8', 'B8', 'C8', 'D8', 'E8', 'F8', 'G8', 'H8'],\n",
    "#         ['A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7'],\n",
    "#         ['A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6'],\n",
    "#         ['A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5', 'H5'],\n",
    "#         ['A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4'],\n",
    "#         ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3'],\n",
    "#         ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2'],\n",
    "#         ['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1']\n",
    "#     ])\n",
    "\n",
    "#     char_to_int = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8} \n",
    "\n",
    "#     int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "#     encoded_board = [(char_to_int[pos[0]], int(pos[1])) for pos in np.ndarray.flatten(board)]\n",
    "\n",
    "#     new_board = [tuple(map(sum, zip((char_to_int[pos[0]], int(pos[1])), move))) for pos in np.ndarray.flatten(board)]\n",
    " \n",
    "#     moves = []\n",
    "#     for pos, new_pos in zip(np.ndarray.flatten(board), new_board):\n",
    "#         try:\n",
    "# #             print(pos, new_pos)\n",
    "#             if new_pos[1] > 8:\n",
    "#                 raise Exception()\n",
    "            \n",
    "#             if move[2] is None:\n",
    "#                 move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}'\n",
    "#             else:\n",
    "#                 move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}{move[2]}'\n",
    "            \n",
    "#             if '-' in move_:\n",
    "#                 raise Exception()\n",
    "            \n",
    "#             if '0' in move_:\n",
    "#                 raise Exception()\n",
    "                \n",
    "#         except Exception:\n",
    "#             move_ = 'XXXX'\n",
    "#         moves.append(move_)\n",
    "#     return np.array(moves).reshape(8, 8)\n",
    "\n",
    "# moves = {\n",
    "#     # Queen Moves\n",
    "#     'Q N1': (0, 1, None),\n",
    "#     'Q N2': (0, 2, None),\n",
    "#     'Q N3': (0, 3, None),\n",
    "#     'Q N4': (0, 4, None),\n",
    "#     'Q N5': (0, 5, None),\n",
    "#     'Q N6': (0, 6, None),\n",
    "#     'Q N7': (0, 7, None),\n",
    "#     'Q NE1': (1, 1, None),\n",
    "#     'Q NE2': (2, 2, None),\n",
    "#     'Q NE3': (3, 3, None),\n",
    "#     'Q NE4': (4, 4, None),\n",
    "#     'Q NE5': (5, 5, None),\n",
    "#     'Q NE6': (6, 6, None),\n",
    "#     'Q NE7': (7, 7, None),\n",
    "#     'Q E1': (1, 0, None),\n",
    "#     'Q E2': (2, 0, None),\n",
    "#     'Q E3': (3, 0, None),\n",
    "#     'Q E4': (4, 0, None),\n",
    "#     'Q E5': (5, 0, None),\n",
    "#     'Q E6': (6, 0, None),\n",
    "#     'Q E7': (7, 0, None),\n",
    "#     'Q SE1': (1, -1, None),\n",
    "#     'Q SE2': (2, -2, None),\n",
    "#     'Q SE3': (3, -3, None),\n",
    "#     'Q SE4': (4, -4, None),\n",
    "#     'Q SE5': (5, -5, None),\n",
    "#     'Q SE6': (6, -6, None),\n",
    "#     'Q SE7': (7, -7, None),\n",
    "#     'Q S1': (0, -1, None),\n",
    "#     'Q S2': (0, -2, None),\n",
    "#     'Q S3': (0, -3, None),\n",
    "#     'Q S4': (0, -4, None),\n",
    "#     'Q S5': (0, -5, None),\n",
    "#     'Q S6': (0, -6, None),\n",
    "#     'Q S7': (0, -7, None),\n",
    "#     'Q SW1': (-1, -1, None),\n",
    "#     'Q SW2': (-2, -2, None),\n",
    "#     'Q SW3': (-3, -3, None),\n",
    "#     'Q SW4': (-4, -4, None),\n",
    "#     'Q SW5': (-5, -5, None),\n",
    "#     'Q SW6': (-6, -6, None),\n",
    "#     'Q SW7': (-7, -7, None),\n",
    "#     'Q W1': (-1, 0, None),\n",
    "#     'Q W2': (-2, 0, None),\n",
    "#     'Q W3': (-3, 0, None),\n",
    "#     'Q W4': (-4, 0, None),\n",
    "#     'Q W5': (-5, 0, None),\n",
    "#     'Q W6': (-6, 0, None),\n",
    "#     'Q W7': (-7, 0, None),\n",
    "#     'Q NW1': (-1, 1, None),\n",
    "#     'Q NW2': (-2, 2, None),\n",
    "#     'Q NW3': (-3, 3, None),\n",
    "#     'Q NW4': (-4, 4, None),\n",
    "#     'Q NW5': (-5, 5, None),\n",
    "#     'Q NW6': (-6, 6, None),\n",
    "#     'Q NW7': (-7, 7, None),\n",
    "#     # Knight Moves\n",
    "#     '2N1E' : (1, 2, None),\n",
    "#     '1N2E' : (2, 1, None),\n",
    "#     '1S2E' : (2, -1, None),\n",
    "#     '2S1E' : (1, -2, None),\n",
    "#     '2S1W' : (-1, -2, None),\n",
    "#     '1S2W' : (-2, -1, None),\n",
    "#     '1N2W' : (-2, 1, None),\n",
    "#     '2N1W' : (-1, 2, None),\n",
    "#     # Promtions\n",
    "#     'PQ N1' : (0, 1, 'q'),\n",
    "#     'PR N1' : (0, 1, 'r'),\n",
    "#     'PB N1' : (0, 1, 'b'),\n",
    "#     'PN N1' : (0, 1, 'n'),\n",
    "#     'PQ NE1' : (1, 1, 'q'),\n",
    "#     'PR NE1' : (1, 1, 'r'),\n",
    "#     'PB NE1' : (1, 1, 'b'),\n",
    "#     'PN NE1' : (1, 1, 'n'),\n",
    "#     'PQ NW1' : (-1, 1, 'q'),\n",
    "#     'PR NW1' : (-1, 1, 'r'),\n",
    "#     'PB NW1' : (-1, 1, 'b'),\n",
    "#     'PN NW1' : (-1, 1, 'n')\n",
    "# }\n",
    "\n",
    "# new_boards = []\n",
    "# for key in moves.keys():\n",
    "#     print(f'Move {key}')\n",
    "#     new_board = move_board(moves[key])\n",
    "#     new_boards.append(new_board)\n",
    "#     print(new_board, '\\n')\n",
    "    \n",
    "# new_boards = np.array(new_boards)\n",
    "# new_boards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logits = env.action\n",
    "# logits = np.random.rand(len(moves), 8, 8)\n",
    "\n",
    "# print(env.action_size)\n",
    "# new_boards_ = np.ndarray.flatten(new_boards)\n",
    "# logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "# move_logits = [(new_boards_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "# move_logits = dict(move_logits)\n",
    "\n",
    "# legal_actions = ['g1h3','g1f3','b1c3','b1a3','h2h3','g2g3','f2f3','e2e3','d2d3','c2c3','b2b3','a2a3','h2h4','g2g4','f2f4','e2e4','d2d4','c2c4','b2b4','a2a4']\n",
    "\n",
    "# legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in legal_actions}\n",
    "# legal_move_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Logits FOR CONTROLLER TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEGAL_MULTIPIER = 20\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# legal_move_logits_ = [legal_move_logits for _ in range(BATCH_SIZE)] \n",
    "# logits = np.random.rand(BATCH_SIZE, len(moves), 8, 8)\n",
    "# logits_ = np.swapaxes(logits.reshape(-1, BATCH_SIZE), 0, 1)\n",
    "# new_boards__ = [[x.lower() for x in list(new_boards_)] for _ in range(BATCH_SIZE)]\n",
    "# target_policies = []\n",
    "# for idx in range(BATCH_SIZE):\n",
    "#     new_boards___ = new_boards__[idx]\n",
    "#     logits__ = logits_[idx]\n",
    "#     legal_move_logits__ = {k.lower(): v * LEGAL_MULTIPIER for k, v in legal_move_logits_[idx].items()}\n",
    "#     for move, legal_move_logit in legal_move_logits__.items():\n",
    "#         index = new_boards___.index(move)\n",
    "#         logits__[index] += legal_move_logit\n",
    "#     target_policies.append(logits__)\n",
    "# target_policies = np.array(target_policies).reshape(BATCH_SIZE, len(moves), 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data for VAE and figure out action size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vae_training_data(games):\n",
    "#     \"\"\"Creates training date for CNN-VAE\"\"\"\n",
    "#     actions = {}\n",
    "#     states = {}\n",
    "#     for game in range(1, games + 1):\n",
    "#         env.reset()\n",
    "#         state, legal_actions = env.observe()\n",
    "#         encoded_state = env.encode()\n",
    "#         states[encoded_state] = state\n",
    "#         while not env.terminal_test():\n",
    "#             action = random.choice(legal_actions)            \n",
    "#             state, legal_actions = env.step(action)\n",
    "#             encoded_state = env.encode()\n",
    "#             states[encoded_state] = state\n",
    "# #             actions[encoded_state] = agent.one_hot_encode_action(action)\n",
    "#         print(f'Game {game} | Unique States {len(states.keys())} Unique Actions {len(actions)}', end = '\\r')\n",
    "    \n",
    "#     validate_path('data/vae')\n",
    "#     with open('data/vae/states.pkl', 'wb') as file:\n",
    "#         pickle.dump(states, file)\n",
    "#     with open('data/vae/actions.pkl', 'wb') as file:\n",
    "#         pickle.dump(actions, file)\n",
    "    \n",
    "# create_vae_training_data(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_vae_(name, starting_channels, filename, z_size, epochs, batch_size):\n",
    "#     \"\"\"Trains the CNN-VAE\"\"\"\n",
    "#     params = {\n",
    "#         'z_size' : z_size,\n",
    "#         'batch_size' : batch_size,\n",
    "#         'learning_rate' : 1e-4,\n",
    "#         'kl_tolerance' : 0.5,\n",
    "#         'batch_norm' : True,\n",
    "#         'starting_channels' : starting_channels\n",
    "#     }\n",
    "\n",
    "#     vae = CNN_AE(name, params, False)\n",
    "#     with open(f'data/vae/{filename}',  'rb') as pickle_file:\n",
    "#         data = list(pickle.load(pickle_file).values())\n",
    "#         data = [np.array(data_) for data_ in data]\n",
    "    \n",
    "#     train_ae(vae, data, epochs, 100, 2)\n",
    "\n",
    "    \n",
    "# train_vae_('Test', env.state_size[0], 'states.pkl', 300, 1000, 200)\n",
    "# # train_vae_('Test Actions', env.action_size[0], 'actions.pkl', 600, 100, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_vae(name, starting_channels, filename, z_size):\n",
    "#     \"\"\"Visualize Test of the CNN-VAE\"\"\"\n",
    "    \n",
    "#     with open(f'data/vae/{filename}',  'rb') as pickle_file:\n",
    "#         data = list(pickle.load(pickle_file).values())\n",
    "#         data = [np.array(data_) for data_ in data]\n",
    "    \n",
    "#     vae = CNN_AE(name, None, 'Latest')\n",
    "#     vae = vae.to(vae.device)\n",
    "\n",
    "#     test_loader = DataLoader(\n",
    "#         torch.tensor(data).float(),\n",
    "#         batch_size = vae.batch_size, \n",
    "#         shuffle = True\n",
    "#     )\n",
    "#     count = 0\n",
    "    \n",
    "#     for batch_idx, inputs in enumerate(test_loader):\n",
    "#         if inputs.shape[0] != vae.batch_size:\n",
    "#             continue\n",
    "#         inputs = inputs.to(vae.device)\n",
    "#         outputs = vae(inputs)\n",
    "#         axes = []\n",
    "#         for idx in range(inputs.shape[0]):\n",
    "#             sample_input = transforms.ToPILImage(mode = 'RGB')(inputs[idx].squeeze(0).detach().cpu())\n",
    "#             sample_output = transforms.ToPILImage(mode = 'RGB')(outputs[idx].squeeze(0).detach().cpu()) \n",
    "#             fig, ax = plt.subplots(1,2)\n",
    "#             ax[0].imshow(sample_input)\n",
    "#             ax[1].imshow(sample_output)\n",
    "#             plt.show()\n",
    "# #         columns = 5\n",
    "# #         for i, images_ in enumerate(images):\n",
    "\n",
    "# #             plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "# #             plt.imshow(image)\n",
    "            \n",
    "    \n",
    "    \n",
    "# test_vae('Test', env.state_size[0], 'test_states.pkl', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data for MDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mdn_training_data(name, games):\n",
    "#     \"\"\"Create training data for MDN-RNN\"\"\"\n",
    "#     vae = CNN_AE(name, None, 'Latest')\n",
    "#     vae.eval()\n",
    "#     data = pd.DataFrame(columns = ['Game ID', 'Sequence #', 'State ID'])\n",
    "#     rollouts = []\n",
    "#     transitions = 0\n",
    "    \n",
    "#     for game in range(1, games + 1):\n",
    "#         white_states = []\n",
    "#         white_actions = []\n",
    "#         black_states = []\n",
    "#         black_actions = []\n",
    "#         env.reset()\n",
    "    \n",
    "#         state, legal_actions = env.observe()\n",
    "#         while not env.terminal_test():\n",
    "#             action = random.choice(legal_actions)\n",
    "#             state, legal_actions = env.step(action)\n",
    "#             transitions += 1\n",
    "#             if env.whites_turn:\n",
    "#                 white_states.append(state)\n",
    "#                 white_actions.append(agent.one_hot_encode_action(action))\n",
    "#             else:\n",
    "#                 black_states.append(state)\n",
    "#                 black_actions.append(agent.one_hot_encode_action(action))             \n",
    "            \n",
    "#         white_zs = vae.encode(torch.tensor(white_states).float())\n",
    "#         white_actions = torch.tensor(white_actions).float().squeeze(-1)\n",
    "#         black_zs = vae.encode(torch.tensor(black_states).float())\n",
    "#         black_actions = torch.tensor(black_actions).float().squeeze(-1)        \n",
    "        \n",
    "# #         white_zas = torch.cat([white_zs, white_actions], dim = 1)\n",
    "# #         black_zas = torch.cat([black_zs, black_actions], dim = 1)\n",
    "#         rollouts.append((white_zs.detach().numpy(), white_actions.detach().numpy()))\n",
    "#         rollouts.append((black_zs.detach().numpy(), black_actions.detach().numpy()))\n",
    "        \n",
    "#         print(f'Game {game} Transitions {transitions}', end = '\\r')\n",
    "    \n",
    "#     validate_path('data/mdn')\n",
    "#     with open('data/mdn/rollouts.pkl', 'wb') as file:\n",
    "#         pickle.dump(rollouts, file)\n",
    "    \n",
    "    \n",
    "# create_mdn_training_data('Test', 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training_mdn(vae_name, name):\n",
    "#     torch.cuda.empty_cache() \n",
    "#     \"\"\"Trains the MDN-RNN\"\"\"\n",
    "#     epochs = 499\n",
    "#     batch_size = 5\n",
    "    \n",
    "#     params = {\n",
    "#         'hidden_size' : 150,\n",
    "#         'gaussian_size' : 3,\n",
    "#         'stacked_layers': 1,\n",
    "#         'grad_clip' : 0.5,\n",
    "#         'learning_rate' : 1e-4\n",
    "#     }\n",
    "    \n",
    "#     with open(f'data/mdn/rollouts.pkl', 'rb') as pickle_file:\n",
    "#         rollouts = pickle.load(pickle_file)\n",
    "    \n",
    "#     params['z_size'] = rollouts[0][0].shape[1]\n",
    "#     params['action_size'] = rollouts[0][1].shape[1]\n",
    "#     params['batch_size'] = batch_size\n",
    "#     vae = CNN_AE(vae_name, None, 'Latest')\n",
    "#     vae.eval()\n",
    "#     vae = vae.to(vae.device)\n",
    "#     mdn = MDN_RNN(name, params, False)\n",
    "    \n",
    "#     mdn.train()\n",
    "#     mdn = mdn.to(mdn.device)\n",
    "#     optimizer = Adam(mdn.parameters(), lr = mdn.learning_rate)\n",
    "        \n",
    "#     inputs = []\n",
    "#     prev_states = []\n",
    "#     targets = []\n",
    "#     seq_lengths = []\n",
    "    \n",
    "# #     print(\"ROLLOUTS\", len(rollouts))\n",
    "#     for rollout in rollouts:\n",
    "#         zs, actions = rollout\n",
    "#         zas = np.concatenate((zs, actions), axis = 1)\n",
    "#         seq_lengths.append(zas[ : -1].shape[0])\n",
    "#         inputs.append(zas[ : -1])\n",
    "#         targets.append(zs[1 : ])\n",
    "#         prev_states.append(zs[ : -1])\n",
    "        \n",
    "#     idxs = range(len(rollouts))\n",
    "    \n",
    "#     # Sorts order from longest to shortest sequence\n",
    "#     idxs = [x for _, x in sorted(zip(seq_lengths, idxs), reverse = True)]\n",
    "#     seq_lengths = [seq_lengths[x] for x in idxs]\n",
    "#     inputs = [np.array(inputs[x]) for x in idxs]\n",
    "#     targets = [np.array(targets[x]) for x in idxs]\n",
    "#     prev_states = [np.array(prev_states[x]) for x in idxs]\n",
    "    \n",
    "#     max_seq_len = max(seq_lengths)\n",
    "#     rollout_size = len(seq_lengths)\n",
    "# #     print(\"ROLLOUT SIZE\", rollout_size, \"MAX SEQ\", max_seq_len)\n",
    "    \n",
    "#     padded_X = np.zeros((rollout_size, max_seq_len, inputs[0].shape[-1]))\n",
    "#     padded_Y = np.zeros((rollout_size, max_seq_len, targets[0].shape[-1]))\n",
    "#     padded_Z = np.zeros((rollout_size, max_seq_len, prev_states[0].shape[-1]))\n",
    "# #     https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "    \n",
    "#     for i, x_len in enumerate(seq_lengths):\n",
    "#         padded_X[i, 0 : x_len] = inputs[i]\n",
    "#         padded_Y[i, 0 : x_len] = targets[i]\n",
    "#         padded_Z[i, 0 : x_len] = prev_states[i]\n",
    "        \n",
    "#     inputs = torch.tensor(padded_X).float().squeeze(1).to(mdn.device)\n",
    "#     targets = torch.tensor(padded_Y).float().squeeze(1).to(mdn.device)\n",
    "#     prev_states = torch.tensor(padded_Z).float().squeeze(1).to(mdn.device)\n",
    "    \n",
    "# #     print(\"X1\", inputs.shape, targets.shape)\n",
    "#     if inputs.shape[0] % batch_size != 0:\n",
    "#         inputs = inputs[inputs.shape[0] // batch_size, :, :]\n",
    "# #     print(\"X2\", inputs.shape, targets.shape)    \n",
    "#     inputs = inputs.reshape(-1, batch_size, max_seq_len, mdn.z_size + mdn.action_size)\n",
    "#     targets = targets.reshape(-1, batch_size, max_seq_len, mdn.z_size)\n",
    "#     prev_states = prev_states.reshape(-1, batch_size, max_seq_len, mdn.z_size)\n",
    "# #     print(\"X3\", inputs.shape, targets.shape)  \n",
    "    \n",
    "#     seq_lengths = list(torch.tensor(seq_lengths).reshape(-1, batch_size).numpy())\n",
    "    \n",
    "#     for epoch in range(epochs + 1):\n",
    "#         for batch in range(inputs.shape[0]):\n",
    "#             inputs_ = inputs[batch]\n",
    "#             targets_ = targets[batch]\n",
    "#             prev_states_ = prev_states[batch]\n",
    "#             seq_lengths_ = list(seq_lengths[batch])\n",
    "            \n",
    "#             targets_ = torch.nn.utils.rnn.pack_padded_sequence(targets_, seq_lengths_, batch_first = True)\n",
    "#             targets_, _ = torch.nn.utils.rnn.pad_packed_sequence(targets_, batch_first = True)\n",
    "\n",
    "#             # Set initial hidden and cell states\n",
    "#             hidden = mdn.init_hidden(batch_size)\n",
    "\n",
    "#             # Forward pass\n",
    "#             hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "#             (pi, mu, sigma), hidden = mdn(inputs_, hidden, seq_lengths_)\n",
    "#             loss = loss_function(targets_, pi, mu, sigma)\n",
    "            \n",
    "#             outputs_ = torch.stack([torch.normal(mu, sigma)[:, :, i, :] for i in range(mdn.gaussian_size)]).float()\n",
    "            \n",
    "#             random_sample = random.randrange(targets_.shape[1])\n",
    "# #             print(\"TARGETS\", targets_.shape, \"OUTPUTS\", outputs_.shape, \"PREV STATES\", prev_states_.shape, \"RANDOM\", random_sample)\n",
    "            \n",
    "#             target_d = targets_[0][random_sample].unsqueeze(0).to(vae.device)\n",
    "#             output_d = outputs_[: , 0, random_sample].to(vae.device)\n",
    "#             prev_state_d = prev_states_[0][random_sample].unsqueeze(0).to(vae.device)\n",
    "# #             print(\"TARGET D\", target_d.shape, \"OUTPUT D\", output_d.shape, \"PREV STATE D\", prev_state_d.shape, \"RANDOM\", random_sample)\n",
    "            \n",
    "#             output_d = [output_d[x].unsqueeze(0) for x in range(output_d.shape[0])]\n",
    "# #             print(\"FLAT OUTPUT D\", len(output_d), output_d[0].shape)\n",
    "#             x = torch.stack([prev_state_d, target_d] + output_d).squeeze(1).float()\n",
    "# #             print(\"DECODE X\", x.shape)\n",
    "#             compare_x = vae.decode(x)\n",
    "            \n",
    "#             sample_prev_state = compare_x[0].squeeze(0).detach().cpu()\n",
    "#             sample_target = compare_x[1].squeeze(0).detach().cpu()\n",
    "#             sample_outputs = [compare_x[idx].squeeze(0).detach().cpu() for idx in range(1, compare_x.shape[0])] \n",
    "            \n",
    "#             sample_target = transforms.ToPILImage(mode = 'RGB')(sample_target)\n",
    "#             sample_prev_state = transforms.ToPILImage(mode = 'RGB')(sample_prev_state)\n",
    "#             sample_outputs = [transforms.ToPILImage(mode = 'RGB')(sample_output) for sample_output in sample_outputs]\n",
    "            \n",
    "#             fig, ax = plt.subplots(1, compare_x.shape[0], figsize = (15, 15))\n",
    "#             ax[0].imshow(sample_prev_state)\n",
    "#             ax[1].imshow(sample_target)\n",
    "#             for idx, sample_output in enumerate(sample_outputs):\n",
    "#                 ax[1 + idx].imshow(sample_output)\n",
    "#             plt.show()\n",
    "            \n",
    "# #             print(compare_x.shape)\n",
    "#             # Backward and optimize\n",
    "#             mdn.zero_grad()\n",
    "#             loss.backward()\n",
    "#             clip_grad_norm_(mdn.parameters(), mdn.grad_clip)\n",
    "#             optimizer.step()\n",
    "#         print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, epochs, loss.item()))\n",
    "            \n",
    "#         mdn.save_model(epoch)\n",
    "# training_mdn('Test', 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA GO TIPS \n",
    "\n",
    "\n",
    "## MCTS\n",
    "Each simulation proceeds by\n",
    "selecting in each state s a move a with low visit count, high move probability and high value\n",
    "(averaged over the leaf states of simulations that selected a from s) according to the current\n",
    "neural network fθ. The search returns a vector π representing a probability distribution over\n",
    "moves, either proportionally or greedily with respect to the visit counts at the root state.\n",
    "The parameters θ of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters θ. Games are played by selecting\n",
    "moves for both players by MCTS, at ∼ πt\n",
    ". At the end of the game, the terminal position sT is\n",
    "scored according to the rules of the game to compute the game outcome z: −1 for a loss, 0 for\n",
    "a draw, and +1 for a win. The neural network parameters θ are updated so as to minimise the\n",
    "error between the predicted outcome vt and the game outcome z, and to maximise the similarity\n",
    "of the policy vector pt\n",
    "to the search probabilities πt\n",
    ". Specifically, the parameters θ are adjusted\n",
    "by gradient descent on a loss function l that sums over mean-squared error and cross-entropy\n",
    "losses respectively\n",
    "\n",
    "(p, v) = fθ(s)                    \n",
    "\n",
    "l = (z − v) 2 − π > log p + c||θ||2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env.decoder, env.action_size, 'Test', 'Test', 'Test', test_mode = False)\n",
    "\n",
    "agent.reset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.decode(b'\\x80\\x03}q\\x00(X\\x05\\x00\\x00\\x00boardq\\x01}q\\x02(K?cchess\\nPiece\\nq\\x03)\\x81q\\x04}q\\x05(X\\n\\x00\\x00\\x00piece_typeq\\x06K\\x04X\\x05\\x00\\x00\\x00colorq\\x07\\x89ubK9h\\x03)\\x81q\\x08}q\\t(h\\x06K\\x02h\\x07\\x89ubK6h\\x03)\\x81q\\n}q\\x0b(h\\x06K\\x06h\\x07\\x89ubK0h\\x03)\\x81q\\x0c}q\\r(h\\x06K\\x01h\\x07\\x89ubK.h\\x03)\\x81q\\x0e}q\\x0f(h\\x06K\\x01h\\x07\\x88ubK)h\\x03)\\x81q\\x10}q\\x11(h\\x06K\\x01h\\x07\\x89ubK\\x1fh\\x03)\\x81q\\x12}q\\x13(h\\x06K\\x02h\\x07\\x88ubK\\x1eh\\x03)\\x81q\\x14}q\\x15(h\\x06K\\x03h\\x07\\x89ubK\\x1dh\\x03)\\x81q\\x16}q\\x17(h\\x06K\\x06h\\x07\\x88ubK\\x1ch\\x03)\\x81q\\x18}q\\x19(h\\x06K\\x01h\\x07\\x89ubK\\x1ah\\x03)\\x81q\\x1a}q\\x1b(h\\x06K\\x01h\\x07\\x88ubK\\x18h\\x03)\\x81q\\x1c}q\\x1d(h\\x06K\\x02h\\x07\\x89ubK\\x17h\\x03)\\x81q\\x1e}q\\x1f(h\\x06K\\x01h\\x07\\x88ubK\\x01h\\x03)\\x81q }q!(h\\x06K\\x02h\\x07\\x88ubuX\\x04\\x00\\x00\\x00turnq\"\\x88X\\r\\x00\\x00\\x00legal_actionsq#]q$(X\\x04\\x00\\x00\\x00h4f5q%X\\x04\\x00\\x00\\x00h4f3q&X\\x04\\x00\\x00\\x00h4g2q\\'X\\x04\\x00\\x00\\x00f4g5q(X\\x04\\x00\\x00\\x00f4e5q)X\\x04\\x00\\x00\\x00f4g4q*X\\x04\\x00\\x00\\x00f4e4q+X\\x04\\x00\\x00\\x00f4g3q,X\\x04\\x00\\x00\\x00f4e3q-X\\x04\\x00\\x00\\x00b1c3q.X\\x04\\x00\\x00\\x00b1a3q/X\\x04\\x00\\x00\\x00b1d2q0X\\x04\\x00\\x00\\x00h3g4q1X\\x04\\x00\\x00\\x00c4c5q2eu.')\n",
    "print(env.legal_actions())\n",
    "env.board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"Monte Carlo Tree Search Algorithm geared for Neural Networks\"\"\"\n",
    "\n",
    "    def __init__(self, env, agent, mcts_simulations = 100, max_depth = 100, delta = 0.5):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.cpuct = 0.2    # WARNING BULLSHIT NUMBER!\n",
    "        self.delta = delta  # value to prevent crash if no edges are visited\n",
    "        self.mcts_simulations = mcts_simulations\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.Qsa = {}       # stores Q values for s, a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores # times edge s, a was visited\n",
    "        self.Ns = {}        # stores # times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores victory result (1, 0, -1) ended for board s\n",
    "        self.Vs = {}        # stores legal actions for board s\n",
    "        self.zh = {}        # stores preserved state that is fed into the controller\n",
    "\n",
    "    def action_probabilities(self, encoded_state, temp = 1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for runs in range(self.mcts_simulations):\n",
    "#             print(f\"{Fore.BLUE}MCTS SIMULATION {runs + 1}{Style.RESET_ALL}\")\n",
    "            self.search(encoded_state, 0)\n",
    "        \n",
    "        s = encoded_state\n",
    "        self.env.decode(encoded_state)\n",
    "        legal_actions = self.env.legal_actions()\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in legal_actions]\n",
    "        # counts array represent the number of time each action edge from your current state was traversed\n",
    "\n",
    "        if temp == 0: # temprature is 0 representing taking the best action possible (greedy)\n",
    "            bestA = np.argmax(counts) # bestA: best action number : argmax Returns the indices of the maximum values\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            action_probs = {legal_actions[idx] : probs[idx] for idx in range(len(legal_actions))}\n",
    "            return action_probs # returns the definite move(s) with same greedy reward, out of which one move HAS to be played\n",
    "        \n",
    "#         print(f\"{Fore.BLUE}COUNTS {sum(counts)} {counts}{Style.RESET_ALL}\")\n",
    "        \n",
    "        # Handles frequent draw situation when MCTS fails to explore when the game is over resulting 0 counts causing div 0 error\n",
    "        if sum(counts) == 0:\n",
    "            counts = [1 for _ in counts]\n",
    "        counts = [x ** (1. / temp) + 0.5 for x in counts]\n",
    "        probs = [x / float(sum(counts)) for x in counts]\n",
    "        action_probs = {legal_actions[idx] : probs[idx] for idx in range(len(legal_actions))}\n",
    "        \n",
    "        # If the game isn't over return the state zh or return None at the end\n",
    "#         if self.env.result() != 0:\n",
    "#             return action_probs, None\n",
    "        return action_probs, self.zh[s]  \n",
    "        #returns the probablity of different moves that CAN be played resulting in uniform distribution\n",
    "\n",
    "\n",
    "    def search(self, encoded_state, depth):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        s = deepcopy(encoded_state)\n",
    "        self.env.decode(encoded_state)\n",
    "#         print(f\"{Fore.GREEN}SEARCH DEPTH {depth}{Style.RESET_ALL}\")\n",
    "        \n",
    "        # Check if its an terminal state, -1 Opponent Won, 0 Game not Over, 1 Player Won\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.env.result()\n",
    "        if self.Es[s] != 0 or self.env.terminal_test():\n",
    "            state, _ = self.env.observe()\n",
    "            self.zh[s] = self.agent.encode_zh(state)\n",
    "            \n",
    "#             print(\"SIMULATION OVER!\", self.env.terminal_test(), -self.Es[s], self.env.board.result())\n",
    "            return -self.Es[s]\n",
    "        \n",
    "        state, legal_actions = self.env.observe()\n",
    "#         legal_actions = self.env.legal_actions()\n",
    "\n",
    "        if s not in self.Ps: #if the current state 's' is not explored/expanded before n=0 by MCTS then create a new node and rollout\n",
    "            self.Ps[s], v, self.zh[s] = self.agent.act(state, legal_actions)\n",
    "#             print(\"self.Ps[s] at Depth\", depth, \"\\n\", self.Ps[s])\n",
    "            valids = legal_actions\n",
    "\n",
    "            self.Vs[s] = valids \n",
    "            self.Ns[s] = 0\n",
    "#             print(\"VALUE\", -v)\n",
    "            return -v\n",
    "        \n",
    "        valids = self.Vs[s] #as already visited the valid moves array 'Vs' is already initialized\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "        \n",
    "        self.env.decode(encoded_state)\n",
    "        legal_actions = self.env.legal_actions()\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        \n",
    "        agent_actions = list(self.Ps[s].keys())\n",
    "        \n",
    "        # Handles the occasional legal action that isn't an actual legal action\n",
    "        # Pawn side cut even though there is no enemy piece in respective position\n",
    "#         print(\"XXXXX PRE LEGAL ACTIONS\\n\", legal_actions)\n",
    "        if not set(agent_actions) == set(legal_actions):\n",
    "            legal_actions = agent_actions\n",
    "#         print(\"XXXXX POST LEGAL ACTIONS\\n\", legal_actions)\n",
    "        for a in legal_actions:\n",
    "            if (s,a) in self.Qsa:\n",
    "                u = self.Qsa[(s,a)] + self.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (1 + self.Nsa[(s,a)])\n",
    "#                 print(\"In Qsa\")\n",
    "            else:\n",
    "                self.Ps[s][a] \n",
    "                u = self.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s])     # Q = 0 ? : node exists but not explored as added and initilized during nnet phase\n",
    "#                 print(\"Not In Qsa\", self.Ps[s][a], self.Ns[s], math.sqrt(self.Ns[s]))\n",
    "#             print(\"U\", u)\n",
    "            if math.isnan(u):\n",
    "                u = 0\n",
    "#                 print(\"Override U\", u)\n",
    "            if u > cur_best:\n",
    "                cur_best = u\n",
    "                best_act = a\n",
    "#         print(f\"{Fore.GREEN}BEST ACTION {best_act}{Style.RESET_ALL}\")\n",
    "        a = best_act\n",
    "\n",
    "        _, _ = self.env.step(a)\n",
    "        encoded_next_state = self.env.encode()\n",
    "\n",
    "        v = self.search(encoded_next_state, depth + 1) # RECURSION until leaf node or terminal node is found\n",
    "        \n",
    "        self.env.decode(encoded_state)\n",
    "        \n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)] * self.Qsa[(s,a)] + v) / (self.Nsa[(s,a)] + 1) #update the Q Value\n",
    "            self.Nsa[(s,a)] += 1 # increment number of visits to this node in MCTS\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v # initialize the new node\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the tracked information\"\"\"\n",
    "        self.Qsa = {}       \n",
    "        self.Nsa = {}       \n",
    "        self.Ns = {}        \n",
    "        self.Ps = {}       \n",
    "        self.Es = {}\n",
    "        self.Vs = {}  \n",
    "        self.zh = {}\n",
    "        self.hidden = {}\n",
    "        self.agent.reset(1)\n",
    "        self.env.reset()\n",
    "\n",
    "# for _ in range(5):\n",
    "#     env.reset()\n",
    "#     mcts_white = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = 20)\n",
    "#     mcts_black = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = 20)\n",
    "#     game_move = 0\n",
    "#     while not env.terminal_test():\n",
    "#     #     print(f\"{Fore.RED}GAME MOVE {game_move}{Style.RESET_ALL}\")\n",
    "#         encoded_state = env.encode()\n",
    "#         if env.whites_turn: \n",
    "#             action_probs, zh = mcts_white.action_probabilities(encoded_state)\n",
    "#         else:\n",
    "#             action_probs, zh = mcts_black.action_probabilities(encoded_state)\n",
    "        \n",
    "        \n",
    "# #         print(game_move, env.whites_turn, env.terminal_test(), type(zh))\n",
    "# #         if zh == None:\n",
    "# #             print(encoded_state)\n",
    "#         action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "# #         print(f\"{Fore.CYAN}ACTUAL MOVE\\n {action_probs}{Style.RESET_ALL}\")\n",
    "#         _, _ = env.step(action)\n",
    "    \n",
    "#         game_move += 1    \n",
    "#     print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! GAME COMPLETED !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COPY THIS\n",
    "# # https://github.com/suragnair/alpha-zero-general/blob/master/Coach.py\n",
    "\n",
    "# env.reset()\n",
    "# mcts_white = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = 20)\n",
    "# mcts_black = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = 20)\n",
    "# game_move = 0\n",
    "\n",
    "# while not env.terminal_test():\n",
    "#     print(f\"{Fore.RED}GAME MOVE {game_move}{Style.RESET_ALL}\")\n",
    "#     encoded_state = env.encode()\n",
    "#     if env.whites_turn: \n",
    "#         action_probs = mcts_white.action_probabilities(encoded_state)\n",
    "#     else:\n",
    "#         action_probs = mcts_black.action_probabilities(encoded_state)\n",
    "#     action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "#     print(f\"{Fore.CYAN}ACTUAL MOVE\\n {action_probs}{Style.RESET_ALL}\")\n",
    "#     _, _ = env.step(action)\n",
    "    \n",
    "#     game_move += 1\n",
    "        \n",
    "# print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! GAME COMPLETED !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.decode(b'\\x80\\x03}q\\x00(X\\x05\\x00\\x00\\x00boardq\\x01}q\\x02(K<cchess\\nPiece\\nq\\x03)\\x81q\\x04}q\\x05(X\\n\\x00\\x00\\x00piece_typeq\\x06K\\x02X\\x05\\x00\\x00\\x00colorq\\x07\\x88ubK9h\\x03)\\x81q\\x08}q\\t(h\\x06K\\x06h\\x07\\x89ubK8h\\x03)\\x81q\\n}q\\x0b(h\\x06K\\x04h\\x07\\x89ubK5h\\x03)\\x81q\\x0c}q\\r(h\\x06K\\x01h\\x07\\x89ubK1h\\x03)\\x81q\\x0e}q\\x0f(h\\x06K\\x01h\\x07\\x89ubK0h\\x03)\\x81q\\x10}q\\x11(h\\x06K\\x01h\\x07\\x89ubK)h\\x03)\\x81q\\x12}q\\x13(h\\x06K\\x01h\\x07\\x88ubK&h\\x03)\\x81q\\x14}q\\x15(h\\x06K\\x01h\\x07\\x89ubK\\x1dh\\x03)\\x81q\\x16}q\\x17(h\\x06K\\x01h\\x07\\x89ubK\\x1ch\\x03)\\x81q\\x18}q\\x19(h\\x06K\\x01h\\x07\\x88ubK\\x1bh\\x03)\\x81q\\x1a}q\\x1b(h\\x06K\\x01h\\x07\\x89ubK\\x1ah\\x03)\\x81q\\x1c}q\\x1d(h\\x06K\\x01h\\x07\\x88ubK\\x19h\\x03)\\x81q\\x1e}q\\x1f(h\\x06K\\x02h\\x07\\x89ubK\\x14h\\x03)\\x81q }q!(h\\x06K\\x03h\\x07\\x89ubK\\x0ch\\x03)\\x81q\"}q#(h\\x06K\\x03h\\x07\\x88ubK\\x08h\\x03)\\x81q$}q%(h\\x06K\\x01h\\x07\\x88ubK\\x06h\\x03)\\x81q&}q\\'(h\\x06K\\x04h\\x07\\x89ubK\\x05h\\x03)\\x81q(}q)(h\\x06K\\x06h\\x07\\x88ubK\\x01h\\x03)\\x81q*}q+(h\\x06K\\x02h\\x07\\x88ubK\\x00h\\x03)\\x81q,}q-(h\\x06K\\x04h\\x07\\x88ubuX\\x04\\x00\\x00\\x00turnq.\\x88X\\r\\x00\\x00\\x00legal_actionsq/]q0u.')\n",
    "# env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning. It uses the functions defined\n",
    "    in Game and NeuralNet. args are specified in main.py.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent, params):\n",
    "        self.iterations = params['iterations']\n",
    "        self.episodes = params['episodes']\n",
    "        self.queue_length = params['queue_length']\n",
    "        self.env = deepcopy(env)\n",
    "        self.agent = agent\n",
    "        self.mcts_white = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = params['simulations'])\n",
    "        self.mcts_black = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = params['simulations'])\n",
    "        self.trainExamplesHistory = []  # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.games = 0\n",
    "        self.writer = SummaryWriter(f'runs/{self.controller.name}/{datetime.now()}/{self.games}')\n",
    "\n",
    "    def executeEpisode(self, writer):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
    "        uses temp=0.\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard, currPlayer, pi,v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, else -1.\n",
    "        \"\"\"\n",
    "        experiences = []\n",
    "        self.env.reset()\n",
    "        # RESET HIDDEN STATES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        game_move = 0\n",
    "        while not self.env.terminal_test():\n",
    "            encoded_state = self.env.encode()\n",
    "            if self.env.whites_turn: \n",
    "                action_probs, zh = self.mcts_white.action_probabilities(encoded_state, writer)\n",
    "            else:\n",
    "                action_probs, zh = self.mcts_black.action_probabilities(encoded_state)\n",
    "            if zh is not None:\n",
    "                print(\"ZH NOT NONE\")\n",
    "                experiences.append([zh.detach().cpu().numpy(), self.env.whites_turn, action_probs, None])\n",
    "\n",
    "            else:\n",
    "                print(\"ZH NONE)\n",
    "            action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "            _, _ = self.env.step(action)\n",
    "            \n",
    "            if not self.env.whites_turn: \n",
    "                      \n",
    "    \n",
    "            game_move += 1\n",
    "        \n",
    "        reward = self.env.result()\n",
    "        print(f\"Game {self.games} Score {reward}\")\n",
    "        experiences_ = []\n",
    "        DISCOUNT_GAMMA = 0.95\n",
    "        \n",
    "        for idx, experience in enumerate(experiences):\n",
    "            reward_ = reward\n",
    "            # calculates the discounted return from the reward.. \n",
    "            # / 2 becauses it contains experiences of both black and white\n",
    "            return_ = reward_ * (DISCOUNT_GAMMA ** (len(experiences) - idx) / 2)\n",
    "            if experience[1] != self.env.whites_turn and (reward_ == 1 or reward_ == -1):\n",
    "                return_ *= -1\n",
    "            experiences_.append((experience[0], experience[2], return_))\n",
    "        return experiences_\n",
    "    \n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "        # deletes historical experiences so AI to enable online learning\n",
    "        self.trainExamplesHistory = []\n",
    "        self.writer.games = self.games\n",
    "        \n",
    "        for i in range(1, self.iterations + 1):\n",
    "            iterationTrainExamples = deque([], maxlen = self.queue_length)\n",
    "            for _ in tnrange(self.episodes, desc = \"Self Play\"):\n",
    "                iterationTrainExamples += self.executeEpisode(deepcopy(self.writer))\n",
    "                self.games += 1\n",
    "                self.writer.games += 1\n",
    "\n",
    "            # save the iteration examples to the history \n",
    "            self.trainExamplesHistory.append(iterationTrainExamples)\n",
    "\n",
    "        # shuffle examples before training\n",
    "        trainExamples = []\n",
    "        for e in self.trainExamplesHistory:\n",
    "            trainExamples.extend(e)\n",
    "        shuffle(trainExamples)\n",
    "        print(len(trainExamples))\n",
    "        \n",
    "        self.agent.train(trainExamples, deepcopy(self.writer))\n",
    "        self.mcts_white.agent = deepcopy(self.agent)\n",
    "        self.mcts_black.agent = deepcopy(self.agent)\n",
    "        \n",
    "        self.mcts_white.reset()  # reset search tree as agents brain is now different\n",
    "        self.mcts_black.reset()\n",
    "        \n",
    "\n",
    "#         # training new network, keeping a copy of the old one\n",
    "#         self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "#         self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "#         pmcts = MCTS(self.game, self.pnet, self.args)\n",
    "\n",
    "#         \n",
    "#         nmcts = MCTS(self.game, self.nnet, self.args)\n",
    "\n",
    "#         log.info('PITTING AGAINST PREVIOUS VERSION')\n",
    "#         arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n",
    "#                           lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n",
    "#         pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n",
    "\n",
    "#         log.info('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "#         if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.args.updateThreshold:\n",
    "#             log.info('REJECTING NEW MODEL')\n",
    "#             self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "#         else:\n",
    "#             log.info('ACCEPTING NEW MODEL')\n",
    "#             self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n",
    "#             self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')\n",
    "        \n",
    "\n",
    "params = {\n",
    "    'iterations': 1,\n",
    "    'episodes': 50,\n",
    "    'queue_length' : 500,\n",
    "    'simulations' : 4\n",
    "}\n",
    "# coach = Coach(env, agent, params)\n",
    "# for _ in range(500):\n",
    "#     coach.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWindow(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setGeometry(100, 100, 1100, 1100)\n",
    "\n",
    "        self.widgetSvg = QSvgWidget(parent=self)\n",
    "        self.widgetSvg.setGeometry(10, 10, 1080, 1080)\n",
    "\n",
    "\n",
    "# board = chess.Board()\n",
    "# app = QApplication([])\n",
    "# window = MainWindow()\n",
    "# window.show()\n",
    "# app.exec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plotter\n",
    "agent = Agent(env.decoder, env.action_size, 'Test', 'Test', 'Test', test_mode = True)\n",
    "agent.reset(1)\n",
    "\n",
    "class Arena:\n",
    "    \"\"\"An Arena where you can have two players fight\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env        \n",
    "        self.agent = agent\n",
    "        self.root = tk.Tk()\n",
    "        self.root.withdraw()\n",
    "        self.app = QApplication([])\n",
    "        self.window = MainWindow()\n",
    "    \n",
    "    def vs_human(self):\n",
    "        \"\"\"Fight Against the AI\"\"\"\n",
    "        self.agent = Agent(self.env.decoder, self.env.action_size, 'Test', 'Test', 'Test', test_mode = False)\n",
    "        self.mcts_black = MCTS(deepcopy(self.env), deepcopy(self.agent), mcts_simulations = 20)\n",
    "        \n",
    "        self.env.reset()\n",
    "        self.mcts_black.reset()\n",
    "        game_move = 0\n",
    "        while not self.env.terminal_test():\n",
    "            if not self.env.whites_turn:\n",
    "                encoded_state = self.env.encode()\n",
    "                action_probs, zh = self.mcts_black.action_probabilities(encoded_state)\n",
    "                \n",
    "                figureObject, axesObject = plotter.subplots()\n",
    "                # Draw the pie chart\n",
    "                axesObject.pie(list(action_probs.values()), labels = list(action_probs.keys()), autopct = '%1.2f', startangle = 90)\n",
    "                # Aspect ratio - equal means pie is a circl\n",
    "                axesObject.axis('equal')\n",
    "                plotter.show()\n",
    "                \n",
    "                \n",
    "                action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "            else:\n",
    "                legal_actions = self.env.legal_actions() \n",
    "                self.render()\n",
    "                action = simpledialog.askstring(title = \"Move\", prompt = f\"What's your next move?\\n{legal_actions}\")\n",
    "                while action not in legal_actions:\n",
    "                    action = simpledialog.askstring(title = \"Move\", prompt = f\"Invalid Move. What's your next move?\\n{legal_actions}\")\n",
    "                    if action is None:\n",
    "                        break\n",
    "                if action is None:\n",
    "                    break\n",
    "            \n",
    "            _, _ = self.env.step(action)\n",
    "            game_move += 1   \n",
    "        print(env.result())\n",
    "        \n",
    "    def render(self):\n",
    "        \"\"\"Renders the chess board\"\"\"\n",
    "        chessboardSvg = chess.svg.board(self.env.board).encode(\"UTF-8\")\n",
    "        self.window.widgetSvg.load(chessboardSvg)\n",
    "        self.window.show()\n",
    "        self.app.exec()\n",
    "        \n",
    "        \n",
    "arena = Arena(env, agent)\n",
    "arena.vs_human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-trader]",
   "language": "python",
   "name": "conda-env-conda-trader-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
