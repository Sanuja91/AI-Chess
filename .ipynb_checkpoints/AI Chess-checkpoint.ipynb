{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPMIND ALPHAZERO CHESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Encode actions\n",
    "- Create training data for MDN-LSTM\n",
    "- Pretrain MDN-LSTM\n",
    "- Create controller (MDN-LSTM should be trainable)\n",
    "- Link Controller - Monte Carlo Tree Search \n",
    "- Create Self play class\n",
    "- Create Arena class\n",
    "\n",
    "DEEP MIND OPEN ACCESS PAPER \n",
    "\n",
    "https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd\n",
    "\n",
    "DISTRIBUTED IMPLEMENTATION\n",
    "\n",
    "https://github.com/mokemokechicken/reversi-alpha-zero/blob/master/src/reversi_zero/lib/ggf.py\n",
    "\n",
    "CHESS MOVES\n",
    "\n",
    "https://www.ichess.net/blog/chess-pieces-moves/\n",
    "\n",
    "BOARD REPRESENTATIONS\n",
    "\n",
    "https://medium.com/datadriveninvestor/reconstructing-chess-positions-f195fd5944e\n",
    "\n",
    "ALPHA ZERO EXPLANATION\n",
    "\n",
    "https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess, gym, pickle, random, torch, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy, deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "from utilities import *\n",
    "# import constants \n",
    "from constants import *\n",
    "from models.vae import CNN_VAE, train_vae\n",
    "\n",
    "# REFERENCES\n",
    "# Tensorflow implementation for Chess\n",
    "# https://github.com/saurabhk7/chess-alpha-zero\n",
    "\n",
    "# Pytorch implementation for Connect4\n",
    "# https://github.com/plkmo/AlphaZero_Connect4/tree/master/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['A7A8' 'B7B8' 'C7C8' ... 'F7F8' 'G7G8' 'H7H8']\n",
      "  ['A6A7' 'B6B7' 'C6C7' ... 'F6F7' 'G6G7' 'H6H7']\n",
      "  ...\n",
      "  ['A3A4' 'B3B4' 'C3C4' ... 'F3F4' 'G3G4' 'H3H4']\n",
      "  ['A2A3' 'B2B3' 'C2C3' ... 'F2F3' 'G2G3' 'H2H3']\n",
      "  ['A1A2' 'B1B2' 'C1C2' ... 'F1F2' 'G1G2' 'H1H2']]\n",
      "\n",
      " [['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['A6A8' 'B6B8' 'C6C8' ... 'F6F8' 'G6G8' 'H6H8']\n",
      "  ...\n",
      "  ['A3A5' 'B3B5' 'C3C5' ... 'F3F5' 'G3G5' 'H3H5']\n",
      "  ['A2A4' 'B2B4' 'C2C4' ... 'F2F4' 'G2G4' 'H2H4']\n",
      "  ['A1A3' 'B1B3' 'C1C3' ... 'F1F3' 'G1G3' 'H1H3']]\n",
      "\n",
      " [['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ...\n",
      "  ['A3A6' 'B3B6' 'C3C6' ... 'F3F6' 'G3G6' 'H3H6']\n",
      "  ['A2A5' 'B2B5' 'C2C5' ... 'F2F5' 'G2G5' 'H2H5']\n",
      "  ['A1A4' 'B1B4' 'C1C4' ... 'F1F4' 'G1G4' 'H1H4']]\n",
      "\n",
      " ...\n",
      "\n",
      " [['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['XXXX' 'B7A8r' 'C7B8r' ... 'F7E8r' 'G7F8r' 'H7G8r']\n",
      "  ['XXXX' 'B6A7r' 'C6B7r' ... 'F6E7r' 'G6F7r' 'H6G7r']\n",
      "  ...\n",
      "  ['XXXX' 'B3A4r' 'C3B4r' ... 'F3E4r' 'G3F4r' 'H3G4r']\n",
      "  ['XXXX' 'B2A3r' 'C2B3r' ... 'F2E3r' 'G2F3r' 'H2G3r']\n",
      "  ['XXXX' 'B1A2r' 'C1B2r' ... 'F1E2r' 'G1F2r' 'H1G2r']]\n",
      "\n",
      " [['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['XXXX' 'B7A8b' 'C7B8b' ... 'F7E8b' 'G7F8b' 'H7G8b']\n",
      "  ['XXXX' 'B6A7b' 'C6B7b' ... 'F6E7b' 'G6F7b' 'H6G7b']\n",
      "  ...\n",
      "  ['XXXX' 'B3A4b' 'C3B4b' ... 'F3E4b' 'G3F4b' 'H3G4b']\n",
      "  ['XXXX' 'B2A3b' 'C2B3b' ... 'F2E3b' 'G2F3b' 'H2G3b']\n",
      "  ['XXXX' 'B1A2b' 'C1B2b' ... 'F1E2b' 'G1F2b' 'H1G2b']]\n",
      "\n",
      " [['XXXX' 'XXXX' 'XXXX' ... 'XXXX' 'XXXX' 'XXXX']\n",
      "  ['XXXX' 'B7A8n' 'C7B8n' ... 'F7E8n' 'G7F8n' 'H7G8n']\n",
      "  ['XXXX' 'B6A7n' 'C6B7n' ... 'F6E7n' 'G6F7n' 'H6G7n']\n",
      "  ...\n",
      "  ['XXXX' 'B3A4n' 'C3B4n' ... 'F3E4n' 'G3F4n' 'H3G4n']\n",
      "  ['XXXX' 'B2A3n' 'C2B3n' ... 'F2E3n' 'G2F3n' 'H2G3n']\n",
      "  ['XXXX' 'B1A2n' 'C1B2n' ... 'F1E2n' 'G1F2n' 'H1G2n']]]\n"
     ]
    }
   ],
   "source": [
    "class Chess_Environment(gym.Env):\n",
    "    \"\"\"Chess Environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.white_pieces = ['P', 'N', 'B', 'R', 'Q', 'K']\n",
    "        self.black_pieces = [piece.lower() for piece in self.white_pieces]\n",
    "        self.x_coords, self.y_coords = np.meshgrid(list(range(0, 8)), list(range(0, 8)))\n",
    "        self.x_coords = self.x_coords / 7\n",
    "        self.y_coords = self.y_coords / 7\n",
    "        self.state_size = self.observe().shape\n",
    "#         self.action_size = 1968\n",
    "        self.init_action_decoder()\n",
    "        self.whites_turn = True\n",
    "        print(self.decoder)\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment\"\"\"\n",
    "        self.board = chess.Board()\n",
    "        \n",
    "    def terminal_test(self):\n",
    "        \"\"\"Checks if the game is over\"\"\"\n",
    "        return self.board.is_game_over()\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\"Gives the end game result\"\"\"\n",
    "        if not self.terminal_test():\n",
    "            raise Exception('Game is not finished!')\n",
    "        \n",
    "        result = self.board.result()\n",
    "        if result == '1-0':\n",
    "            return 1\n",
    "        elif result == '0-1':\n",
    "            return -1\n",
    "        return 0\n",
    "        \n",
    "    def legal_actions(self):\n",
    "        \"\"\"Provides a list of legal actions in current state\"\"\"\n",
    "        return [str(legal_action) for legal_action in list(self.board.legal_moves)]\n",
    "    \n",
    "    def encode(self):\n",
    "        \"\"\"Encodes game state into a string\"\"\"\n",
    "        board_ = self.board.piece_map()\n",
    "            \n",
    "        encoded = {\n",
    "            'board' : board_,\n",
    "            'turn' : self.board.turn\n",
    "        }\n",
    "        \n",
    "        return pickle.dumps(encoded)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Decodes string into game state and sets board and turn\"\"\"\n",
    "        decoded = pickle.loads(encoded)\n",
    "        self.board.set_piece_map(decoded['board'])\n",
    "        if decoded['turn'] == 0:\n",
    "            self.board = self.board.mirror()\n",
    "        self.board.turn = decoded['turn']\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"Create observation from the game state\"\"\"\n",
    "\n",
    "        board_ = copy(self.board)\n",
    "            \n",
    "        board_ = np.ndarray.flatten(np.array(board_.__str__().split())).reshape(8, 8)\n",
    "        \n",
    "        black_pawns = np.isin(copy(board_), ['p']).astype(int)\n",
    "        black_knights = np.isin(copy(board_), ['n']).astype(int)\n",
    "        black_rooks = np.isin(copy(board_), ['r']).astype(int)\n",
    "        black_bishops = np.isin(copy(board_), ['b']).astype(int)\n",
    "        black_queen = np.isin(copy(board_), ['q']).astype(int)\n",
    "        black_king = np.isin(copy(board_), ['k']).astype(int)\n",
    "         \n",
    "        white_pawns = np.isin(copy(board_), ['P']).astype(int)\n",
    "        white_knights = np.isin(copy(board_), ['N']).astype(int)\n",
    "        white_rooks = np.isin(copy(board_), ['R']).astype(int)\n",
    "        white_bishops = np.isin(copy(board_), ['B']).astype(int)\n",
    "        white_queen = np.isin(copy(board_), ['Q']).astype(int)\n",
    "        white_king = np.isin(copy(board_), ['K']).astype(int)\n",
    "        \n",
    "        state = np.array([\n",
    "            white_pawns,\n",
    "            white_knights,\n",
    "            white_rooks,\n",
    "            white_bishops,\n",
    "            white_queen,\n",
    "            white_king,\n",
    "            black_pawns,\n",
    "            black_knights,\n",
    "            black_rooks,\n",
    "            black_bishops,\n",
    "            black_queen,\n",
    "            black_king\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Perform a step in the environment\"\"\"\n",
    "        action = self.select_action(actions)\n",
    "        self.board.push_uci(action)\n",
    "        self.board = self.board.mirror()\n",
    "        if self.whites_turn:\n",
    "            self.whites_turn = False\n",
    "        else:\n",
    "            self.whites_turn = True\n",
    "        return self.observe()\n",
    "    \n",
    "    def move_board(self, move):\n",
    "        \"\"\"Moves the board positions as per the move\"\"\"\n",
    "\n",
    "        char_to_int = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8} \n",
    "\n",
    "        int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "        encoded_board = [(char_to_int[pos[0]], int(pos[1])) for pos in np.ndarray.flatten(BOARD)]\n",
    "\n",
    "        new_board = [tuple(map(sum, zip((char_to_int[pos[0]], int(pos[1])), move))) for pos in np.ndarray.flatten(BOARD)]\n",
    "        moves = []\n",
    "        for pos, new_pos in zip(np.ndarray.flatten(BOARD), new_board):\n",
    "            try:\n",
    "                if new_pos[1] > 8:\n",
    "                    raise Exception()\n",
    "                if move[2] is None:\n",
    "                    move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}'\n",
    "                else:\n",
    "                    move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}{move[2]}'\n",
    "            \n",
    "                if '-' in move_:\n",
    "                    raise Exception()\n",
    "            \n",
    "                if '0' in move_:\n",
    "                    raise Exception()\n",
    "                \n",
    "            except Exception:\n",
    "                move_ = 'XXXX'\n",
    "            moves.append(move_)   \n",
    "        return np.array(moves).reshape(8, 8)\n",
    "    \n",
    "    def init_action_decoder(self):\n",
    "        \"\"\"Initialize the decoder to decode the actions\"\"\"\n",
    "        decoder = []\n",
    "        for key in MOVES.keys():\n",
    "            decoder_ = self.move_board(MOVES[key])\n",
    "            decoder.append(decoder_)\n",
    "#             print(f'Move {key}\\n', new_board, '\\n')\n",
    "    \n",
    "        self.decoder = np.array(decoder)\n",
    "        self.action_size = self.decoder.shape\n",
    "        \n",
    "    def select_action(self, logits):\n",
    "        \"\"\"Decodes the output from the NN to legal actions\"\"\" \n",
    "        decoder_ = np.ndarray.flatten(self.decoder)\n",
    "        logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "        move_logits = [(decoder_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "        move_logits = dict(move_logits)\n",
    "        \n",
    "        legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in self.legal_actions()}\n",
    "        probabilities = list(legal_move_logits.values()) / sum(list(legal_move_logits.values()))\n",
    "\n",
    "        action = random.choices(list(legal_move_logits.keys()), weights = probabilities, k = 1)[0]\n",
    "        return action\n",
    "\n",
    "env = Chess_Environment()\n",
    "# env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10 | Unqiue States 4169 Unique Actions 4168\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observe()\n",
    "\n",
    "state_planes = [\n",
    "    'White Pawns',\n",
    "    'White Knights',\n",
    "    'White Rooks',\n",
    "    'White Bishops',\n",
    "    'White Queen',\n",
    "    'White King',\n",
    "    'Black Pawns',\n",
    "    'Black Knights',\n",
    "    'Black Rooks',\n",
    "    'Black Bishops',\n",
    "    'Black Queen',\n",
    "    'Black King'\n",
    "]\n",
    "\n",
    "for inx, state_ in enumerate(state):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(state_planes[inx], fontsize = 20)\n",
    "    xticks = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    yticks = list(range(1, 9))\n",
    "    yticks.reverse()\n",
    "    plt.xticks(list(range(0, 8)), xticks)\n",
    "    plt.yticks(list(range(0, 8)), yticks)\n",
    "    plt.imshow(state_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
    "queen_distance = list(range(1, 8))\n",
    "queen_moves = [f'{direction}{distance}' for direction in queen_directions for distance in queen_distance]\n",
    "\n",
    "knight_moves = ['2N1E', '1N2E', '1S2E', '2S1E', '2S1W', '1S2W', '1N2W', '2N1W']\n",
    "underpromotion_moves = ['DOUBLEM', 'NECUT', 'NWCUT']\n",
    "moves = queen_moves + knight_moves + underpromotion_moves\n",
    "\n",
    "print(f'{len(queen_moves)} QUEEN MOVES {queen_moves[: 15]}\\n{len(knight_moves)} KNIGHT MOVES {knight_moves}\\n{len(underpromotion_moves)} UNDERPROMOTION MOVES {underpromotion_moves}')\n",
    "print(f'\\nTOTAL MOVES {len(moves)}')\n",
    "print(moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_board(move):\n",
    "    \"\"\"Moves the board positions as per the move\"\"\"\n",
    "    print(move)\n",
    "    board = np.array([\n",
    "        ['A8', 'B8', 'C8', 'D8', 'E8', 'F8', 'G8', 'H8'],\n",
    "        ['A7', 'B7', 'C7', 'D7', 'E7', 'F7', 'G7', 'H7'],\n",
    "        ['A6', 'B6', 'C6', 'D6', 'E6', 'F6', 'G6', 'H6'],\n",
    "        ['A5', 'B5', 'C5', 'D5', 'E5', 'F5', 'G5', 'H5'],\n",
    "        ['A4', 'B4', 'C4', 'D4', 'E4', 'F4', 'G4', 'H4'],\n",
    "        ['A3', 'B3', 'C3', 'D3', 'E3', 'F3', 'G3', 'H3'],\n",
    "        ['A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2', 'H2'],\n",
    "        ['A1', 'B1', 'C1', 'D1', 'E1', 'F1', 'G1', 'H1']\n",
    "    ])\n",
    "\n",
    "    char_to_int = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8} \n",
    "\n",
    "    int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "    encoded_board = [(char_to_int[pos[0]], int(pos[1])) for pos in np.ndarray.flatten(board)]\n",
    "\n",
    "    new_board = [tuple(map(sum, zip((char_to_int[pos[0]], int(pos[1])), move))) for pos in np.ndarray.flatten(board)]\n",
    " \n",
    "    moves = []\n",
    "    for pos, new_pos in zip(np.ndarray.flatten(board), new_board):\n",
    "        try:\n",
    "#             print(pos, new_pos)\n",
    "            if new_pos[1] > 8:\n",
    "                raise Exception()\n",
    "            \n",
    "            if move[2] is None:\n",
    "                move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}'\n",
    "            else:\n",
    "                move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}{move[2]}'\n",
    "            \n",
    "            if '-' in move_:\n",
    "                raise Exception()\n",
    "            \n",
    "            if '0' in move_:\n",
    "                raise Exception()\n",
    "                \n",
    "        except Exception:\n",
    "            move_ = 'XXXX'\n",
    "        moves.append(move_)\n",
    "    return np.array(moves).reshape(8, 8)\n",
    "\n",
    "moves = {\n",
    "    # Queen Moves\n",
    "    'Q N1': (0, 1, None),\n",
    "    'Q N2': (0, 2, None),\n",
    "    'Q N3': (0, 3, None),\n",
    "    'Q N4': (0, 4, None),\n",
    "    'Q N5': (0, 5, None),\n",
    "    'Q N6': (0, 6, None),\n",
    "    'Q N7': (0, 7, None),\n",
    "    'Q NE1': (1, 1, None),\n",
    "    'Q NE2': (2, 2, None),\n",
    "    'Q NE3': (3, 3, None),\n",
    "    'Q NE4': (4, 4, None),\n",
    "    'Q NE5': (5, 5, None),\n",
    "    'Q NE6': (6, 6, None),\n",
    "    'Q NE7': (7, 7, None),\n",
    "    'Q E1': (1, 0, None),\n",
    "    'Q E2': (2, 0, None),\n",
    "    'Q E3': (3, 0, None),\n",
    "    'Q E4': (4, 0, None),\n",
    "    'Q E5': (5, 0, None),\n",
    "    'Q E6': (6, 0, None),\n",
    "    'Q E7': (7, 0, None),\n",
    "    'Q SE1': (1, -1, None),\n",
    "    'Q SE2': (2, -2, None),\n",
    "    'Q SE3': (3, -3, None),\n",
    "    'Q SE4': (4, -4, None),\n",
    "    'Q SE5': (5, -5, None),\n",
    "    'Q SE6': (6, -6, None),\n",
    "    'Q SE7': (7, -7, None),\n",
    "    'Q S1': (0, -1, None),\n",
    "    'Q S2': (0, -2, None),\n",
    "    'Q S3': (0, -3, None),\n",
    "    'Q S4': (0, -4, None),\n",
    "    'Q S5': (0, -5, None),\n",
    "    'Q S6': (0, -6, None),\n",
    "    'Q S7': (0, -7, None),\n",
    "    'Q SW1': (-1, -1, None),\n",
    "    'Q SW2': (-2, -2, None),\n",
    "    'Q SW3': (-3, -3, None),\n",
    "    'Q SW4': (-4, -4, None),\n",
    "    'Q SW5': (-5, -5, None),\n",
    "    'Q SW6': (-6, -6, None),\n",
    "    'Q SW7': (-7, -7, None),\n",
    "    'Q W1': (-1, 0, None),\n",
    "    'Q W2': (-2, 0, None),\n",
    "    'Q W3': (-3, 0, None),\n",
    "    'Q W4': (-4, 0, None),\n",
    "    'Q W5': (-5, 0, None),\n",
    "    'Q W6': (-6, 0, None),\n",
    "    'Q W7': (-7, 0, None),\n",
    "    'Q NW1': (-1, 1, None),\n",
    "    'Q NW2': (-2, 2, None),\n",
    "    'Q NW3': (-3, 3, None),\n",
    "    'Q NW4': (-4, 4, None),\n",
    "    'Q NW5': (-5, 5, None),\n",
    "    'Q NW6': (-6, 6, None),\n",
    "    'Q NW7': (-7, 7, None),\n",
    "    # Knight Moves\n",
    "    '2N1E' : (1, 2, None),\n",
    "    '1N2E' : (2, 1, None),\n",
    "    '1S2E' : (2, -1, None),\n",
    "    '2S1E' : (1, -2, None),\n",
    "    '2S1W' : (-1, -2, None),\n",
    "    '1S2W' : (-2, -1, None),\n",
    "    '1N2W' : (-2, 1, None),\n",
    "    '2N1W' : (-1, 2, None),\n",
    "    # Promtions\n",
    "    'PQ N1' : (0, 1, 'q'),\n",
    "    'PR N1' : (0, 1, 'r'),\n",
    "    'PB N1' : (0, 1, 'b'),\n",
    "    'PN N1' : (0, 1, 'n'),\n",
    "    'PQ NE1' : (1, 1, 'q'),\n",
    "    'PR NE1' : (1, 1, 'r'),\n",
    "    'PB NE1' : (1, 1, 'b'),\n",
    "    'PN NE1' : (1, 1, 'n'),\n",
    "    'PQ NW1' : (-1, 1, 'q'),\n",
    "    'PR NW1' : (-1, 1, 'r'),\n",
    "    'PB NW1' : (-1, 1, 'b'),\n",
    "    'PN NW1' : (-1, 1, 'n')\n",
    "}\n",
    "\n",
    "new_boards = []\n",
    "for key in moves.keys():\n",
    "    print(f'Move {key}')\n",
    "    new_board = move_board(moves[key])\n",
    "    new_boards.append(new_board)\n",
    "    print(new_board, '\\n')\n",
    "    \n",
    "new_boards = np.array(new_boards)\n",
    "new_boards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = env.action\n",
    "# np.random.rand(len(moves), 8, 8)\n",
    "\n",
    "print(env.action_size)\n",
    "new_boards_ = np.ndarray.flatten(new_boards)\n",
    "logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "move_logits = [(new_boards_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "move_logits = dict(move_logits)\n",
    "\n",
    "legal_actions = ['g1h3','g1f3','b1c3','b1a3','h2h3','g2g3','f2f3','e2e3','d2d3','c2c3','b2b3','a2a3','h2h4','g2g4','f2f4','e2e4','d2d4','c2c4','b2b4','a2a4']\n",
    "\n",
    "legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in legal_actions}\n",
    "legal_move_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data for VAE and figure out action size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10 | Unqiue States 2819 Unique Actions 2818\r"
     ]
    }
   ],
   "source": [
    "def create_vae_training_data(games):\n",
    "    \"\"\"Creates training date for CNN-VAE\"\"\"\n",
    "    actions = {}\n",
    "    states = {}\n",
    "    for game in range(1, games + 1):\n",
    "        env.reset()\n",
    "        state = env.observe()\n",
    "        encoded_state = env.encode()\n",
    "        states[encoded_state] = state\n",
    "        while not env.terminal_test():\n",
    "            action = np.random.rand(env.action_size[0], env.action_size[1], env.action_size[2])\n",
    "            env.step(action)\n",
    "            encoded_state = env.encode()\n",
    "            states[encoded_state] = state\n",
    "            actions[encoded_state] = action\n",
    "        print(f'Game {game} | Unqiue States {len(states.keys())} Unique Actions {len(actions)}', end = '\\r')\n",
    "    \n",
    "    validate_path('data/vae')\n",
    "    with open('data/vae/states.pkl', 'wb') as file:\n",
    "        pickle.dump(states, file)\n",
    "    with open('data/vae/actions.pkl', 'wb') as file:\n",
    "        pickle.dump(actions, file)\n",
    "    \n",
    "create_vae_training_data(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Running on cpu\n",
      "\n",
      "CNN_VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(76, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (4): Conv(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (enc_conv_mu): Conv(\n",
      "    (conv): Conv1d(512, 600, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (enc_conv_logvar): Conv(\n",
      "    (conv): Conv1d(512, 600, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (4): Conv(\n",
      "      (conv): ConvTranspose2d(32, 76, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (dec_conv): Conv(\n",
      "    (conv): Conv1d(600, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Train Epoch: 0 [990/2818 (35%)]\tLoss: 3371.780859\n",
      "INPUT torch.Size([76, 8, 8])\n",
      "OUTPUT torch.Size([76, 8, 8])\n",
      "Train Epoch: 0 [1990/2818 (71%)]\tLoss: 3371.817578\n",
      "INPUT torch.Size([76, 8, 8])\n",
      "OUTPUT torch.Size([76, 8, 8])\n",
      "====> Epoch: 0 Average loss: 3362.2297\n"
     ]
    }
   ],
   "source": [
    "def train_vae_(name, starting_channels, filename, z_size):\n",
    "    \"\"\"Trains the CNN-VAE\"\"\"\n",
    "    params = {\n",
    "        'z_size' : z_size,\n",
    "        'batch_size' : 10,\n",
    "        'learning_rate' : 1e-4,\n",
    "        'kl_tolerance' : 0.5,\n",
    "        'batch_norm' : False,\n",
    "        'starting_channels' : starting_channels\n",
    "    }\n",
    "\n",
    "    vae = CNN_VAE(name, params, False)\n",
    "    with open(f'data/vae/{filename}',  'rb') as pickle_file:\n",
    "        data = list(pickle.load(pickle_file).values())\n",
    "        data = [np.array(data_) for data_ in data]\n",
    "    \n",
    "    train_vae(vae, data, 1, 100)\n",
    "\n",
    "    \n",
    "# train_vae_('Test States', env.state_size[0], 'states.pkl', 600)\n",
    "train_vae_('Test Actions', env.action_size[0], 'actions.pkl', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data for MDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/Conv VAE/Test/\n",
      "\n",
      "AI Running on cpu\n",
      "\n",
      "CNN_VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (4): Conv(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (enc_conv_mu): Conv(\n",
      "    (conv): Conv1d(512, 400, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (enc_conv_logvar): Conv(\n",
      "    (conv): Conv1d(512, 400, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (4): Conv(\n",
      "      (conv): ConvTranspose2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (dec_conv): Conv(\n",
      "    (conv): Conv1d(400, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'flatten' requires a 'numpy.ndarray' object but received a 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4302cd3fe370>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m#     torch.save(zs.squeeze(-1), f'{path}\\\\zs.pt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mcreate_mdn_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-4302cd3fe370>\u001b[0m in \u001b[0;36mcreate_mdn_training_data\u001b[1;34m(name, games)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4bb47e797e3a>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;34m\"\"\"Perform a step in the environment\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_uci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmirror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4bb47e797e3a>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, logits)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;34m\"\"\"Decodes the output from the NN to legal actions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mdecoder_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mlogits_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mmove_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'flatten' requires a 'numpy.ndarray' object but received a 'str'"
     ]
    }
   ],
   "source": [
    "def create_mdn_training_data(name, games):\n",
    "    \"\"\"Create training data for MDN-RNN\"\"\"\n",
    "    vae = CNN_VAE(name, None, 'Latest')\n",
    "    vae.eval()\n",
    "    #     \"\"\"Creates training date for MDN-RNN\"\"\"\n",
    "#     games = 50\n",
    "    data = pd.DataFrame(columns = ['Game ID', 'Sequence #', 'State ID'])\n",
    "    rollouts = []\n",
    "    for game in range(1, games + 1):\n",
    "        states = []\n",
    "        actions = []\n",
    "        env.reset()\n",
    "        \n",
    "        state = env.observe()\n",
    "        states.append(state)\n",
    "        while not env.terminal_test():\n",
    "            legal_actions = env.legal_actions()\n",
    "            action = random.choice(legal_actions)\n",
    "            actions.append(action)\n",
    "            state = env.step(action)\n",
    "            states.append(state)\n",
    "        \n",
    "        mus, logvars = vae.encode(torch.tensor(states).float())\n",
    "        zs = vae.reparameterize(mus, logvars)\n",
    "        actions = torch.tensor(actions).float()\n",
    "        \n",
    "        print(type(mus), type(logvars), type(zs), type(actions))\n",
    "        rollouts.append((mus, logvars, zs, actions))\n",
    "        \n",
    "        print(f'Game {game}')\n",
    "#         ' | Unqiue States {len(states.keys())} Unique Actions {len(actions)}', end = '\\r')\n",
    "    \n",
    "#     validate_path('data/vae')\n",
    "#     with open('data/vae/states.pkl', 'wb') as file:\n",
    "#         pickle.dump(states, file)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     path = f'data\\\\inputs\\\\tensors'\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "        \n",
    "#     states = {}\n",
    "#     games = 50\n",
    "#     for game in range(1, games + 1):\n",
    "#         env.reset()\n",
    "\n",
    "#     mus = []\n",
    "#     logvars = []\n",
    "#     zs = []\n",
    "#     for batch_idx, (inputs, _) in enumerate(train_loader):\n",
    "#         mu, logvar = vae.encode(inputs)\n",
    "#         z = vae.reparameterize(mu, logvar)\n",
    "#         mus.append(mu)\n",
    "#         logvars.append(logvar)\n",
    "#         zs.append(z)\n",
    "\n",
    "#     mus = torch.cat(mus)\n",
    "#     logvars = torch.cat(logvars)\n",
    "#     zs = torch.cat(zs)\n",
    "\n",
    "#     torch.save(mus, f'{path}\\\\mus.pt')\n",
    "#     torch.save(logvars, f'{path}\\\\logvars.pt')\n",
    "#     torch.save(zs.squeeze(-1), f'{path}\\\\zs.pt')\n",
    "\n",
    "create_mdn_training_data('Test', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA GO TIPS \n",
    "\n",
    "\n",
    "## MCTS\n",
    "Each simulation proceeds by\n",
    "selecting in each state s a move a with low visit count, high move probability and high value\n",
    "(averaged over the leaf states of simulations that selected a from s) according to the current\n",
    "neural network fθ. The search returns a vector π representing a probability distribution over\n",
    "moves, either proportionally or greedily with respect to the visit counts at the root state.\n",
    "The parameters θ of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters θ. Games are played by selecting\n",
    "moves for both players by MCTS, at ∼ πt\n",
    ". At the end of the game, the terminal position sT is\n",
    "scored according to the rules of the game to compute the game outcome z: −1 for a loss, 0 for\n",
    "a draw, and +1 for a win. The neural network parameters θ are updated so as to minimise the\n",
    "error between the predicted outcome vt and the game outcome z, and to maximise the similarity\n",
    "of the policy vector pt\n",
    "to the search probabilities πt\n",
    ". Specifically, the parameters θ are adjusted\n",
    "by gradient descent on a loss function l that sums over mean-squared error and cross-entropy\n",
    "losses respectively\n",
    "\n",
    "(p, v) = fθ(s)                    \n",
    "\n",
    "l = (z − v) 2 − π > log p + c||θ||2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"Monte Carlo Tree Search Algorithm geared for Neural Networks\"\"\"\n",
    "\n",
    "    def __init__(self, env, agent, mcts_simulations = 25):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.mcts_simulations = mcts_simulations\n",
    "        self.Qsa = {}       # stores Q values for s, a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores # times edge s, a was visited\n",
    "        self.Ns = {}        # stores # times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}        # stores game.getValidMoves for board s\n",
    "\n",
    "    def action_probabilities(self, state, temp = 1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(self.mcts_simulations):\n",
    "            self.search()\n",
    "        \n",
    "        #NOW MAKE THE ACTUAL FINAL MOVE of MCTS by analysing MCTS\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        print('MCTS.py==>getActionProb self.game.stringRepresentation(canonicalBoard) ','s: ', s)\n",
    "\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "        #counts array represent the number of time each action edge from your current state was traversed\n",
    "\n",
    "        if temp==0: #temprature is 0 representing taking the best action possible (greedy)\n",
    "            bestA = np.argmax(counts) #bestA: best action number : argmax Returns the indices of the maximum values\n",
    "            probs = [0]*len(counts)\n",
    "            probs[bestA]=1\n",
    "            return probs #returns the definite move(s) with same greedy reward, out of which one move HAS to be played\n",
    "\n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        print('MCTS.py==>getActionProb returns: probs ','counts: ',counts,'probs: ', probs)\n",
    "        return probs #returns the probablity of different moves that CAN be played resulting in uniform distribution\n",
    "\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_state = self.env.encode()\n",
    "\n",
    "        \"\"\"I don't see a point in this\"\"\"\n",
    "#         if s not in self.Es:\n",
    "#             print(\"ssssssss2\")\n",
    "#             self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "#             #Es :  'end state' array : maps the state string 's' to -1, 0 , 1 to check if the state s is end state\n",
    "#             #-1: Black has won : End state\n",
    "#             #+1: White has won: End state\n",
    "#             # 0: Not an end state ==> continue the search iteration\n",
    "        \n",
    "        \"\"\"I don't see a point in this\"\"\"\n",
    "#         print('MCTS.py==>search ', 'self.Es[s]: ',self.Es[s])\n",
    "#         if self.Es[s]!=0:\n",
    "#             # terminal node : game ended\n",
    "#             print('MCTS.py==>search ', 'Game ended returning: -self.Es[s]: ', -self.Es[s])\n",
    "#             return -self.Es[s]\n",
    "\n",
    "\n",
    "        if s not in self.Ps: #if the current state 's' is not explored/expanded before n=0 by MCTS then create a new node and rollout\n",
    "            #ERRRRRRRRROOOORRRRRRRRRRRRRRRRRRRRRRRRRR never going inside\n",
    "            #If it does not exist, we create a new node in our tree and initialize its\n",
    "            #P (s, ·) = p ~ θ (s) and the expected reward v = v θ (s) from\n",
    "            #our neural network, and initialize Q(s, a) and N (s, a)\n",
    "            #to 0 for all a ==>leaf node wrt current half explored MCTS\n",
    "            print(\"ssssssss3\")\n",
    "\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            print('MCTS.py==>search ', 'valid moves returned by getValidMoves(): ', valids)\n",
    "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves i.e the policies from the policy vector which are useless validmove = 0\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "                print(\"All valid moves were masked, do workaround.\") #print:surag\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids #Vs: valid moves the game board at state 's'\n",
    "            self.Ns[s] = 0\n",
    "            print('MCTS.py==>search ', 'returning value of canonical board -v: ', -v)\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s] #as already visited the valid moves array 'Vs' is already initialized\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s,a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])     # Q = 0 ? : node exists but not explored as added and initilized during nnet phase\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        print ('a: ', a)\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a) #next node in MCTS is selected greedily as we had the policies for that node\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player) #TODO: chess board symmetry issue\n",
    "        print('MCTS.py==>search ', 'best_act: ', a,'next_s: ',next_s,'next_player', next_player)\n",
    "\n",
    "        v = self.search(next_s) #RECURSION until leaf node or terminal node is found\n",
    "\n",
    "        #BACK-UP of MCTS STARTS HERE: after returning from RECURSIVE CALL\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1) #update the Q Value\n",
    "            self.Nsa[(s,a)] += 1 #increment number of visits to this node in MCTS\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v #INITIALIZE the new node\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        print('MCTS.py==>search ', 'returning value of canonical board -v: ', -v)\n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, agent, args):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.args = args\n",
    "        self.Qsa = {}       # stores Q values for s, a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores # times edge s, a was visited\n",
    "        self.Ns = {}        # stores # times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}        # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims): #numMCTSSims: 25 -- bored factor (ref. udacity)\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        #NOW MAKE THE ACTUAL FINAL MOVE of MCTS by analysing MCTS\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        print('MCTS.py==>getActionProb self.game.stringRepresentation(canonicalBoard) ','s: ', s)\n",
    "\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "        #counts array represent the number of time each action edge from your current state was traversed\n",
    "\n",
    "        if temp==0: #temprature is 0 representing taking the best action possible (greedy)\n",
    "            bestA = np.argmax(counts) #bestA: best action number : argmax Returns the indices of the maximum values\n",
    "            probs = [0]*len(counts)\n",
    "            probs[bestA]=1\n",
    "            return probs #returns the definite move(s) with same greedy reward, out of which one move HAS to be played\n",
    "\n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        print('MCTS.py==>getActionProb returns: probs ','counts: ',counts,'probs: ', probs)\n",
    "        return probs #returns the probablity of different moves that CAN be played resulting in uniform distribution\n",
    "\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        print(\"sssssss: \",s)\n",
    "        # s represents the state of the board as a string\n",
    "\n",
    "        if s not in self.Es:\n",
    "            print(\"ssssssss2\")\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "            #Es :  'end state' array : maps the state string 's' to -1, 0 , 1 to check if the state s is end state\n",
    "            #-1: Black has won : End state\n",
    "            #+1: White has won: End state\n",
    "            # 0: Not an end state ==> continue the search iteration\n",
    "\n",
    "        print('MCTS.py==>search ', 'self.Es[s]: ',self.Es[s])\n",
    "        if self.Es[s]!=0:\n",
    "            # terminal node : game ended\n",
    "            print('MCTS.py==>search ', 'Game ended returning: -self.Es[s]: ', -self.Es[s])\n",
    "            return -self.Es[s]\n",
    "\n",
    "\n",
    "        print(\"SELF.PSsss: \", self.Ps)\n",
    "        if s not in self.Ps: #if the current state 's' is not explored/expanded before n=0 by MCTS then create a new node and rollout\n",
    "            #ERRRRRRRRROOOORRRRRRRRRRRRRRRRRRRRRRRRRR never going inside\n",
    "            #If it does not exist, we create a new node in our tree and initialize its\n",
    "            #P (s, ·) = p ~ θ (s) and the expected reward v = v θ (s) from\n",
    "            #our neural network, and initialize Q(s, a) and N (s, a)\n",
    "            #to 0 for all a ==>leaf node wrt current half explored MCTS\n",
    "            print(\"ssssssss3\")\n",
    "\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            print('MCTS.py==>search ', 'valid moves returned by getValidMoves(): ', valids)\n",
    "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves i.e the policies from the policy vector which are useless validmove = 0\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "                print(\"All valid moves were masked, do workaround.\") #print:surag\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids #Vs: valid moves the game board at state 's'\n",
    "            self.Ns[s] = 0\n",
    "            print('MCTS.py==>search ', 'returning value of canonical board -v: ', -v)\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s] #as already visited the valid moves array 'Vs' is already initialized\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s,a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])     # Q = 0 ? : node exists but not explored as added and initilized during nnet phase\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        print ('a: ', a)\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a) #next node in MCTS is selected greedily as we had the policies for that node\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player) #TODO: chess board symmetry issue\n",
    "        print('MCTS.py==>search ', 'best_act: ', a,'next_s: ',next_s,'next_player', next_player)\n",
    "\n",
    "        v = self.search(next_s) #RECURSION until leaf node or terminal node is found\n",
    "\n",
    "        #BACK-UP of MCTS STARTS HERE: after returning from RECURSIVE CALL\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1) #update the Q Value\n",
    "            self.Nsa[(s,a)] += 1 #increment number of visits to this node in MCTS\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v #INITIALIZE the new node\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        print('MCTS.py==>search ', 'returning value of canonical board -v: ', -v)\n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, env, parent = None, parent_action = None):\n",
    "        # parent_action is None for root node\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "        self.env = env\n",
    "        self.parent_action = parent_action\n",
    "        self.unvisited_actions = state.actions()\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.player = state.player()\n",
    "\n",
    "    def is_expandable(self):\n",
    "        \"\"\"Checks if leaf node\"\"\"\n",
    "        return len(self.unvisited_actions) != 0\n",
    "\n",
    "    def has_children(self):\n",
    "        \"\"\"Checks if leaf node\"\"\"\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def is_terminal(self):\n",
    "        \"\"\"Checks if terminal state\"\"\"\n",
    "        return self.env.terminal_test()\n",
    "\n",
    "    def get_value(self):\n",
    "        \"\"\"Gets value of node\"\"\"\n",
    "        if self.visits == 0:\n",
    "            return 0\n",
    "        return self.value / self.visits\n",
    "\n",
    "    def expand(self):\n",
    "        \"\"\"Expands existing leaf node\"\"\"\n",
    "        while len(self.unvisited_actions) > 0:\n",
    "            action = random.choice(self.unvisited_actions)\n",
    "            next_state = self.env.step(action)\n",
    "            self.unvisited_actions.remove(action)\n",
    "            node = Node(state = next_state, parent = self, parent_action = action)\n",
    "            self.children[action] = node\n",
    "        return node\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Monte Carlo Tree Search algorithm\n",
    "    simulations = number of simulations to run\n",
    "    exploration_factor = exploration priority\n",
    "    expert_factory = expert knowledge priority\n",
    "    expansion_threshold = after visiting a leaf node x times,\n",
    "    you expand it without using its rollout value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, simulations, player_id, exploration_factor = 1.4, expert_factor = 2.0, expansion_threshold = 100, rollout_times = 10, simulation_time = 4.9):\n",
    "        self.simulations = simulations\n",
    "        self.exploration_factor = exploration_factor\n",
    "        self.expert_factor = expert_factor\n",
    "        self.expansion_threshold = expansion_threshold\n",
    "        self.rollout_times = rollout_times\n",
    "        self.player_id = player_id\n",
    "        self.simulation_time = simulation_time\n",
    "        self.time = time()\n",
    "\n",
    "    def run(self, state):\n",
    "        root = Node(state)\n",
    "        root.expand()\n",
    "\n",
    "        for _ in range(self.simulations):\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            while not node.is_terminal() and time() - self.time < self.simulation_time * 0.8:\n",
    "\n",
    "                if node.has_children() and node.visits > 0:\n",
    "                    print('Selecting Node')\n",
    "                    node = self.select_next_best_node(node)\n",
    "                    search_path.append(node)\n",
    "                elif node.visits >= self.expansion_threshold and node.is_expandable():\n",
    "                    print('Expanding Node')\n",
    "                    node = node.expand()\n",
    "                    search_path.append(node)\n",
    "                else:\n",
    "                    print('Rolling Out Node')\n",
    "                    node = self.rollout(node, search_path)\n",
    "                    search_path.append(node)\n",
    "\n",
    "        best_next_node = self.select_next_best_node(root)\n",
    "        print('BEST ACTION', best_next_node)\n",
    "        return best_next_node.parent_action\n",
    "\n",
    "    def rollout(self, node, search_path):\n",
    "        \"\"\"Rollouts the states randomly to find approximate value of the node\"\"\"\n",
    "        for action in node.state.actions():\n",
    "            search_path_ = copy(search_path)\n",
    "            search_path_.append(node)\n",
    "            rolled_out_times = 0\n",
    "            while self.rollout_times > rolled_out_times:\n",
    "                state = deepcopy(node.state)\n",
    "                count = 0\n",
    "                while not state.terminal_test():\n",
    "                    count += 1\n",
    "                    state = state.result(random.choice(state.actions()))\n",
    "                    print('SUB LOOP COUNT', count, 'NEXT ACTIONS', len(state.actions()), time() - self.time)\n",
    "                print('EXIT SUB LOOP')\n",
    "                value = state.utility(self.player_id)\n",
    "                if value == float('inf'):\n",
    "                    value = 1\n",
    "                elif value == float('-inf'):\n",
    "                    value = -1\n",
    "                else:\n",
    "                    value = 0                    \n",
    "                self.backpropogate(search_path_, value)\n",
    "                rolled_out_times += 1\n",
    "                print('ROLLED OUT TIMES', rolled_out_times)\n",
    "        return node\n",
    "\n",
    "    def select_next_best_node(self, node):\n",
    "        \"\"\"Picks next child node based on highest value\"\"\"\n",
    "        children = node.children\n",
    "        children_values = {}\n",
    "        for child_key in children.keys():\n",
    "            child = children[child_key]\n",
    "            # value for exploiting the child path\n",
    "            exploitation_value = child.get_value()\n",
    "            # value for exploring the child path\n",
    "            exploration_value = self.exploration_factor * np.log(node.visits) / child.visits\n",
    "            # value for expert knowledge\n",
    "            # expert_value = self.expert_factor * expert_knowledge / (child.visits + 1)\n",
    "\n",
    "            # children_values[child_key] = exploitation_value + exploration_value + expert_value\n",
    "            # print(\"CHILD KEY\", child_key, \"EXPLOIT\", exploitation_value, \"EXPLORE\", exploration_value)\n",
    "            children_values[child_key] = exploitation_value + exploration_value\n",
    "        selected_child_key = max(children_values, key=children_values.get)\n",
    "        print('SELECTED CHILD', selected_child_key)\n",
    "        return children[selected_child_key]\n",
    "\n",
    "    def backpropogate(self, search_path, value):\n",
    "        for node in reversed(search_path):\n",
    "            node.value += value\n",
    "            node.visits += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader",
   "language": "python",
   "name": "trader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
