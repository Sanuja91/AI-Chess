{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# DEEPMIND ALPHAZERO CHESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Fix MCTS UCB Calculation... Pseudocode different from paper\n",
    "- Seperate training and actors.. IMPORTANT! Handle this \n",
    "- Learning rate schedule. 0.2, 0.02, 0.002, 0.0002 after 100k, 300k, 500k games\n",
    "- Have training on GPU and inference CPU\n",
    "- Implement https://www.youtube.com/watch?v=a4VvcmqnkhY\n",
    "- print(f\"INVALID ACTION {action}.\\nSELECTING A RANDOM ACTION FROM LEGAL ACTIONS\\n{self.legal_actions()}\") #### DISABLED FOR VIEWING PRINTS PROPERLY!\n",
    "\n",
    "\n",
    "## TODO LATER\n",
    "#### - Get backward possibly loss to train the controller. Might need to save the tensor output like states, values, etc in MCTS\n",
    "- Create Self play class\n",
    "- Create Arena class\n",
    "\n",
    "DEEP MIND OPEN ACCESS PAPER \n",
    "\n",
    "https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd\n",
    "\n",
    "DEEP MIND OPEN ACCESS PAPER SUPPLEMENTARY MATERIALS\n",
    "\n",
    "https://science.sciencemag.org/content/sci/suppl/2018/12/05/362.6419.1140.DC1/aar6404-Silver-SM.pdf\n",
    "\n",
    "RAY ALPHA ZERO IMPLEMENTATION \n",
    "\n",
    "https://github.com/ray-project/ray/tree/master/rllib/contrib/alpha_zero\n",
    "\n",
    "DUPLICATED MCTS IMPLEMENTATION\n",
    "\n",
    "https://github.com/suragnair/alpha-zero-general/blob/master/MCTS.py\n",
    "\n",
    "DISTRIBUTED IMPLEMENTATION\n",
    "\n",
    "https://github.com/mokemokechicken/reversi-alpha-zero/blob/master/src/reversi_zero/lib/ggf.py\n",
    "\n",
    "CHESS MOVES\n",
    "\n",
    "https://www.ichess.net/blog/chess-pieces-moves/\n",
    "\n",
    "BOARD REPRESENTATIONS\n",
    "\n",
    "https://medium.com/datadriveninvestor/reconstructing-chess-positions-f195fd5944e\n",
    "\n",
    "ALPHA ZERO EXPLANATION\n",
    "\n",
    "https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/\n",
    "\n",
    "TRANSFORMER NETWORK IMPLEMENTATION\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLEL = False\n",
    "\n",
    "if PARALLEL:\n",
    "    !pip install --user python-chess gym colorama tqdm PyQt5 ray tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARALLEL:\n",
    "    import ray\n",
    "    ray.init()\n",
    "\n",
    "import chess, gym, pickle, random, torch, math\n",
    "import chess.svg as svg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy, deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from copy import deepcopy\n",
    "from colorama import init, Fore, Back, Style\n",
    "from collections import Counter, deque\n",
    "from tqdm import tqdm, tnrange, notebook\n",
    "from random import shuffle\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from PyQt5.QtSvg import QSvgWidget\n",
    "from PyQt5.QtWidgets import QApplication, QWidget\n",
    "\n",
    "from utilities import *\n",
    "from constants import *\n",
    "from models.vae import CNN_AE, CNN_VAE, train_ae, train_vae, Conv\n",
    "from models.mdn import MDN_RNN, loss_function, clip_grad_norm_\n",
    "from models.controller import Policy_Controller, Value_Controller\n",
    "plt.style.use('ggplot')\n",
    "# init(convert = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTE CARLO TREE SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"Monte Carlo Tree Search Algorithm geared for Neural Networks\"\"\"\n",
    "\n",
    "    def __init__(self, env, agent, mcts_simulations = 100):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        \n",
    "        # Exploration bonus\n",
    "        self.cpuct_init = 1.25                       \n",
    "        self.cpuct_base = 19652\n",
    "        self.mcts_simulations = mcts_simulations\n",
    "        \n",
    "        self.Qsa = {}                               # stores Q values for s, a (as defined in the paper)\n",
    "        self.Wsa = {}                               # stores the total action value\n",
    "        self.Nsa = {}                               # stores # times edge s, a was visited\n",
    "        self.Ns = {}                                # stores # times board s was visited\n",
    "        self.Ps = {}                                # stores prior probability of selecting a in s\n",
    "\n",
    "        self.Es = {}                                # stores victory result (1, 0, -1) ended for board s\n",
    "        self.z = {}                                 # stores the features of the state\n",
    "        self.hidden = {}                            # stores the hidden states\n",
    "        \n",
    "        # Exploration noise parameters, used in action probabiltiies \n",
    "        self.dirichlet_alpha = 0.3  \n",
    "        self.exploration_fraction = 0.25\n",
    "\n",
    "\n",
    "    def action_probabilities(self, encoded_state, evaluate = False):\n",
    "        \"\"\"\n",
    "        Runs n number of monte carlo simulation, updates the tree and provides\n",
    "        an action selection probability distribution with as well an encoded representation\n",
    "        of the state with historical information zh\n",
    "        \"\"\"\n",
    "        for runs in range(self.mcts_simulations):\n",
    "            self.search(encoded_state, 0)\n",
    "        \n",
    "        s = encoded_state\n",
    "        self.env.decode(encoded_state)\n",
    "        legal_actions = self.env.legal_actions()\n",
    "        \n",
    "        # counts array represent the number of time each action edge from your current state was traversed\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in legal_actions]\n",
    "        \n",
    "        zh = torch.cat((self.z[s].to('cpu'), self.hidden[s][0].squeeze(0).to('cpu')), dim = 1)\n",
    "        # temprature is 0 representing taking the best action possible (greedy)\n",
    "        if evaluate: \n",
    "            # bestA: best action number : argmax Returns the indices of the maximum values\n",
    "            bestA = np.argmax(counts) \n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            action_probs = {legal_actions[idx] : probs[idx] for idx in range(len(legal_actions))}\n",
    "            # returns the definite move(s) with same greedy reward, out of which one move HAS to be played\n",
    "            return action_probs, zh\n",
    "        \n",
    "#         print(f\"{Fore.BLUE}COUNTS {sum(counts)} {counts}{Style.RESET_ALL}\")\n",
    "        \n",
    "        # Handles frequent draw situation when MCTS fails to explore when the game is over resulting 0 counts causing div 0 error\n",
    "        # Dirichlet noise handles this\n",
    "        #         if sum(counts) == 0:\n",
    "#             counts = [1 for _ in counts]\n",
    "        \n",
    "        # Add the dirichlet noise to encourage exploration \n",
    "        noise = np.random.gamma(self.dirichlet_alpha, 1, len(legal_actions))\n",
    "        \n",
    "        frac = self.exploration_fraction\n",
    "        for idx, (a, noise_) in enumerate(zip(legal_actions, noise)):\n",
    "            counts[idx] = counts[idx] * (1 - frac) + noise_ * frac\n",
    "        \n",
    "        probs = [x / float(sum(counts)) for x in counts]\n",
    "        action_probs = {legal_actions[idx] : probs[idx] for idx in range(len(legal_actions))}\n",
    "\n",
    "        return action_probs, zh\n",
    "        #returns the probablity of different moves that CAN be played resulting in uniform distribution\n",
    "\n",
    "\n",
    "    def search(self, encoded_state, depth):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa, Wsa are\n",
    "        updated.\n",
    "        \n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        \"\"\"\n",
    "\n",
    "        s = deepcopy(encoded_state)\n",
    "        self.env.decode(encoded_state)\n",
    "#         print(f\"{Fore.GREEN}SEARCH DEPTH {depth}{Style.RESET_ALL}\")\n",
    "        \n",
    "        # Handles the maintaining of the MDNs hidden states\n",
    "        if s not in self.hidden:\n",
    "            self.hidden[s] = self.agent.get_hidden()\n",
    "        else:\n",
    "            self.agent.load_hidden(self.hidden[s])\n",
    "        \n",
    "        state, legal_actions = self.env.observe()\n",
    "            \n",
    "        # Check if its an terminal state, -1 Opponent Won, 0 Draw or Game not over, 1 Player Won\n",
    "        if self.env.terminal_test():\n",
    "            state, _ = self.env.observe()\n",
    "            \n",
    "            # Stores the AE encoded representation of current state\n",
    "            self.z[s] = self.agent.encode_z(state)\n",
    "            \n",
    "            # Intialize the edges of the state\n",
    "            for a in legal_actions:\n",
    "                self.Wsa[(s,a)] = 0\n",
    "                self.Qsa[(s,a)] = 0\n",
    "                self.Nsa[(s,a)] = 0\n",
    "#             print(\"SIMULATION OVER!\", self.env.terminal_test(), -self.Es[s], self.env.board.result())\n",
    "            return -self.env.result()        \n",
    "\n",
    "        # Intialize the new node at state s with agents policy and value estimate\n",
    "        if s not in self.Ps: \n",
    "            self.Ps[s], v, self.z[s] = self.agent.act(state, legal_actions)\n",
    "#             print(\"self.Ps[s] at Depth\", depth, \"\\n\", self.Ps[s])\n",
    "            self.Ns[s] = 0\n",
    "    \n",
    "            # Intialize the edges of the state\n",
    "            for a in legal_actions:\n",
    "                self.Wsa[(s,a)] = 0\n",
    "                self.Qsa[(s,a)] = 0\n",
    "                self.Nsa[(s,a)] = 0\n",
    "            return -v\n",
    "\n",
    "        self.env.decode(encoded_state)\n",
    "        \n",
    "        # Compare agents action estimates with actual legal actions\n",
    "        legal_actions = self.env.legal_actions()\n",
    "        agent_actions = list(self.Ps[s].keys())\n",
    "        \n",
    "        # Handles the occasional legal action that isn't an actual legal action\n",
    "        # Pawn side cut even though there is no enemy piece in respective position\n",
    "        if not set(agent_actions) == set(legal_actions):\n",
    "            legal_actions = agent_actions\n",
    "\n",
    "        # Update exploration rate as the search gets deeper to explore more\n",
    "        cpuct = math.log((1 + self.Ns[s] + self.cpuct_base) / self.cpuct_base) + self.cpuct_init\n",
    "        cpuct *= math.sqrt(self.Ns[s] / (self.Ns[s] + 1))\n",
    "\n",
    "        # Calculate UCB Upper Confidence Bound based on the \n",
    "        # Value estimate, exploration rate, agents policy and frequency of edge visits\n",
    "        ucb = {}\n",
    "        for a in legal_actions:\n",
    "            ucb[a] = self.Qsa[(s,a)] + cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (self.Nsa[(s,a)] + 1)\n",
    "\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        a = max(ucb, key = ucb.get)\n",
    "        _, _ = self.env.step(a)\n",
    "        \n",
    "        # Update the MDNs hidden state\n",
    "        self.agent.step(self.z[s], a)\n",
    "        \n",
    "        # Reccursive search until leaf node or terminal node is found\n",
    "        encoded_next_state = self.env.encode()\n",
    "        v = self.search(encoded_next_state, depth + 1) \n",
    "        \n",
    "        # Backpropagate through the tree\n",
    "        self.Nsa[(s,a)] += 1 # increment number of visits this edge was taken\n",
    "        self.Wsa[(s,a)] += v # add value to total action value\n",
    "        self.Qsa[(s,a)] = self.Wsa[(s,a)] / self.Nsa[(s,a)] # calculate mean action value\n",
    "        self.Ns[s] += 1      # increment number of visits to this node\n",
    "        return -v\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the tracked information\"\"\"\n",
    "        self.Qsa = {} \n",
    "        self.Wsa = {} \n",
    "        self.Nsa = {}       \n",
    "        self.Ns = {}        \n",
    "        self.Ps = {}       \n",
    "        self.Es = {} \n",
    "        self.zh = {}\n",
    "        self.hidden = {}\n",
    "        self.agent.reset(1)\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"390\" version=\"1.1\" viewBox=\"0 0 390 390\" width=\"390\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><g class=\"white pawn\" id=\"white-pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-linecap=\"round\" stroke-width=\"1.5\" /></g><g class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-knight\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" style=\"fill:#000000; stroke:#000000;\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" /></g><g class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-bishop\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" id=\"white-rook\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" id=\"white-queen\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" id=\"white-king\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g class=\"black pawn\" id=\"black-pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-linecap=\"round\" stroke-width=\"1.5\" /></g><g class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-knight\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" style=\"fill:#ececec; stroke:#ececec;\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-bishop\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" id=\"black-rook\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-linejoin=\"miter\" stroke-width=\"1\" /></g><g class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" id=\"black-queen\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" id=\"black-king\" stroke=\"#000\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"1.5\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect fill=\"#212121\" height=\"390\" width=\"390\" x=\"0\" y=\"0\" /><rect class=\"square dark a1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"330\" /><use transform=\"translate(15, 330)\" xlink:href=\"#white-rook\" /><rect class=\"square light b1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"330\" /><use transform=\"translate(60, 330)\" xlink:href=\"#white-knight\" /><rect class=\"square dark c1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"330\" /><use transform=\"translate(105, 330)\" xlink:href=\"#white-bishop\" /><rect class=\"square light d1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"330\" /><use transform=\"translate(150, 330)\" xlink:href=\"#white-queen\" /><rect class=\"square dark e1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"330\" /><use transform=\"translate(195, 330)\" xlink:href=\"#white-king\" /><rect class=\"square light f1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"330\" /><use transform=\"translate(240, 330)\" xlink:href=\"#white-bishop\" /><rect class=\"square dark g1\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"330\" /><use transform=\"translate(285, 330)\" xlink:href=\"#white-knight\" /><rect class=\"square light h1\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"330\" /><use transform=\"translate(330, 330)\" xlink:href=\"#white-rook\" /><rect class=\"square light a2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"285\" /><use transform=\"translate(15, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark b2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"285\" /><use transform=\"translate(60, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square light c2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"285\" /><use transform=\"translate(105, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark d2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"285\" /><use transform=\"translate(150, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square light e2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"285\" /><use transform=\"translate(195, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark f2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"285\" /><use transform=\"translate(240, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square light g2\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"285\" /><use transform=\"translate(285, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark h2\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"285\" /><use transform=\"translate(330, 285)\" xlink:href=\"#white-pawn\" /><rect class=\"square dark a3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"240\" /><rect class=\"square light b3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"240\" /><rect class=\"square dark c3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"240\" /><rect class=\"square light d3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"240\" /><rect class=\"square dark e3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"240\" /><rect class=\"square light f3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"240\" /><rect class=\"square dark g3\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"240\" /><rect class=\"square light h3\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"240\" /><rect class=\"square light a4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"195\" /><rect class=\"square dark b4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"195\" /><rect class=\"square light c4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"195\" /><rect class=\"square dark d4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"195\" /><rect class=\"square light e4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"195\" /><rect class=\"square dark f4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"195\" /><rect class=\"square light g4\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"195\" /><rect class=\"square dark h4\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"195\" /><rect class=\"square dark a5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"150\" /><rect class=\"square light b5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"150\" /><rect class=\"square dark c5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"150\" /><rect class=\"square light d5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"150\" /><rect class=\"square dark e5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"150\" /><rect class=\"square light f5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"150\" /><rect class=\"square dark g5\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"150\" /><rect class=\"square light h5\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"150\" /><rect class=\"square light a6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"105\" /><rect class=\"square dark b6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"105\" /><rect class=\"square light c6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"105\" /><rect class=\"square dark d6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"105\" /><rect class=\"square light e6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"105\" /><rect class=\"square dark f6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"105\" /><rect class=\"square light g6\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"105\" /><rect class=\"square dark h6\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"105\" /><rect class=\"square dark a7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"60\" /><use transform=\"translate(15, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square light b7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"60\" /><use transform=\"translate(60, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark c7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"60\" /><use transform=\"translate(105, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square light d7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"60\" /><use transform=\"translate(150, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark e7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"60\" /><use transform=\"translate(195, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square light f7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"60\" /><use transform=\"translate(240, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square dark g7\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"60\" /><use transform=\"translate(285, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square light h7\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"60\" /><use transform=\"translate(330, 60)\" xlink:href=\"#black-pawn\" /><rect class=\"square light a8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"15\" y=\"15\" /><use transform=\"translate(15, 15)\" xlink:href=\"#black-rook\" /><rect class=\"square dark b8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"60\" y=\"15\" /><use transform=\"translate(60, 15)\" xlink:href=\"#black-knight\" /><rect class=\"square light c8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"105\" y=\"15\" /><use transform=\"translate(105, 15)\" xlink:href=\"#black-bishop\" /><rect class=\"square dark d8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"150\" y=\"15\" /><use transform=\"translate(150, 15)\" xlink:href=\"#black-queen\" /><rect class=\"square light e8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"195\" y=\"15\" /><use transform=\"translate(195, 15)\" xlink:href=\"#black-king\" /><rect class=\"square dark f8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"240\" y=\"15\" /><use transform=\"translate(240, 15)\" xlink:href=\"#black-bishop\" /><rect class=\"square light g8\" fill=\"#ffce9e\" height=\"45\" stroke=\"none\" width=\"45\" x=\"285\" y=\"15\" /><use transform=\"translate(285, 15)\" xlink:href=\"#black-knight\" /><rect class=\"square dark h8\" fill=\"#d18b47\" height=\"45\" stroke=\"none\" width=\"45\" x=\"330\" y=\"15\" /><use transform=\"translate(330, 15)\" xlink:href=\"#black-rook\" /><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(20, 0) scale(0.75, 0.75)\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(20, 375) scale(0.75, 0.75)\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(65, 0) scale(0.75, 0.75)\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(65, 375) scale(0.75, 0.75)\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(110, 0) scale(0.75, 0.75)\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(110, 375) scale(0.75, 0.75)\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(155, 0) scale(0.75, 0.75)\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(155, 375) scale(0.75, 0.75)\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(200, 0) scale(0.75, 0.75)\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(200, 375) scale(0.75, 0.75)\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(245, 0) scale(0.75, 0.75)\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(245, 375) scale(0.75, 0.75)\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(290, 0) scale(0.75, 0.75)\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(290, 375) scale(0.75, 0.75)\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(335, 0) scale(0.75, 0.75)\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(335, 375) scale(0.75, 0.75)\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 335) scale(0.75, 0.75)\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 335) scale(0.75, 0.75)\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 290) scale(0.75, 0.75)\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 290) scale(0.75, 0.75)\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 245) scale(0.75, 0.75)\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 245) scale(0.75, 0.75)\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 200) scale(0.75, 0.75)\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 200) scale(0.75, 0.75)\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 155) scale(0.75, 0.75)\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 155) scale(0.75, 0.75)\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 110) scale(0.75, 0.75)\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 110) scale(0.75, 0.75)\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 65) scale(0.75, 0.75)\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 65) scale(0.75, 0.75)\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(0, 20) scale(0.75, 0.75)\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g fill=\"#e5e5e5\" stroke=\"#e5e5e5\" transform=\"translate(375, 20) scale(0.75, 0.75)\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g></svg>"
      ],
      "text/plain": [
       "Board('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Chess_Environment(gym.Env):\n",
    "    \"\"\"Chess Environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.white_pieces = ['P', 'N', 'B', 'R', 'Q', 'K']\n",
    "        self.black_pieces = [piece.lower() for piece in self.white_pieces]\n",
    "        self.x_coords, self.y_coords = np.meshgrid(list(range(0, 8)), list(range(0, 8)))\n",
    "        self.x_coords = self.x_coords / 7\n",
    "        self.y_coords = self.y_coords / 7\n",
    "        self.state_size = self.observe()[0].shape\n",
    "        self.init_action_decoder()\n",
    "        self.whites_turn = True\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment\"\"\"\n",
    "        self.board = chess.Board()\n",
    "        \n",
    "    def terminal_test(self):\n",
    "        \"\"\"Checks if the game is over\"\"\"\n",
    "        return self.board.is_game_over(claim_draw = True)\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\"Gives the end game result\"\"\"\n",
    "\n",
    "        result = self.board.result(claim_draw = True)\n",
    "        if result == '1-0':\n",
    "            return 1\n",
    "        elif result == '0-1':\n",
    "            return -1\n",
    "        elif result == '1/2-1/2':\n",
    "            return 0\n",
    "        elif result == '*':\n",
    "            return 0\n",
    "        else:\n",
    "            raise Exception('Invalid Result', result)\n",
    "        \n",
    "    def legal_actions(self):\n",
    "        \"\"\"Provides a list of legal actions in current state\"\"\"\n",
    "        legal_actions = [str(legal_action) for legal_action in list(self.board.legal_moves)]\n",
    "        return legal_actions\n",
    "    \n",
    "    def encode(self):\n",
    "        \"\"\"Encodes game state into a string\"\"\"\n",
    "        board_ = self.board.piece_map()\n",
    "            \n",
    "        encoded = {\n",
    "            'board' : board_,\n",
    "            'turn' : self.board.turn,\n",
    "            'legal_actions' : self.legal_actions()\n",
    "        }\n",
    "        \n",
    "        return pickle.dumps(encoded)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Decodes string into game state and sets board and turn\"\"\"\n",
    "        decoded = pickle.loads(encoded)\n",
    "        self.board.set_piece_map(decoded['board'])\n",
    "        if decoded['turn'] == False:\n",
    "            self.board = self.board.mirror()\n",
    "        self.board.turn = decoded['turn']\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"Create observation from the game state\"\"\"\n",
    "\n",
    "        board_ = copy(self.board)\n",
    "            \n",
    "        board_ = np.ndarray.flatten(np.array(board_.__str__().split())).reshape(8, 8)\n",
    "        \n",
    "        black_pawns = np.isin(copy(board_), ['p']).astype(int)\n",
    "        black_knights = np.isin(copy(board_), ['n']).astype(int)\n",
    "        black_rooks = np.isin(copy(board_), ['r']).astype(int)\n",
    "        black_bishops = np.isin(copy(board_), ['b']).astype(int)\n",
    "        black_queen = np.isin(copy(board_), ['q']).astype(int)\n",
    "        black_king = np.isin(copy(board_), ['k']).astype(int)\n",
    "         \n",
    "        white_pawns = np.isin(copy(board_), ['P']).astype(int)\n",
    "        white_knights = np.isin(copy(board_), ['N']).astype(int)\n",
    "        white_rooks = np.isin(copy(board_), ['R']).astype(int)\n",
    "        white_bishops = np.isin(copy(board_), ['B']).astype(int)\n",
    "        white_queen = np.isin(copy(board_), ['Q']).astype(int)\n",
    "        white_king = np.isin(copy(board_), ['K']).astype(int)\n",
    "        \n",
    "        state = np.array([\n",
    "            white_pawns,\n",
    "            white_knights,\n",
    "            white_rooks,\n",
    "            white_bishops,\n",
    "            white_queen,\n",
    "            white_king,\n",
    "            black_pawns,\n",
    "            black_knights,\n",
    "            black_rooks,\n",
    "            black_bishops,\n",
    "            black_queen,\n",
    "            black_king\n",
    "        ])\n",
    "        \n",
    "        return state, self.legal_actions()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Perform a step in the environment\"\"\"\n",
    "        try:\n",
    "            self.board.push_uci(action)\n",
    "        except ValueError:\n",
    "#             print(f\"INVALID ACTION {action}.\\nSELECTING A RANDOM ACTION FROM LEGAL ACTIONS\\n{self.legal_actions()}\")\n",
    "            actions = random.choices(self.legal_actions())\n",
    "            self.board.push_uci(actions[0])\n",
    "            \n",
    "        self.board = self.board.mirror()\n",
    "        if self.whites_turn:\n",
    "            self.whites_turn = False\n",
    "        else:\n",
    "            self.whites_turn = True\n",
    "        return self.observe()\n",
    "    \n",
    "    def move_board(self, move):\n",
    "        \"\"\"Moves the board positions as per the move\"\"\"\n",
    "\n",
    "        char_to_int = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8} \n",
    "\n",
    "        int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "        encoded_board = [(char_to_int[pos[0]], int(pos[1])) for pos in np.ndarray.flatten(BOARD)]\n",
    "\n",
    "        new_board = [tuple(map(sum, zip((char_to_int[pos[0]], int(pos[1])), move))) for pos in np.ndarray.flatten(BOARD)]\n",
    "        moves = []\n",
    "        for pos, new_pos in zip(np.ndarray.flatten(BOARD), new_board):\n",
    "            try:\n",
    "                if new_pos[1] > 8:\n",
    "                    raise Exception()\n",
    "                if move[2] is None:\n",
    "                    move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}'\n",
    "                else:\n",
    "                    move_ = f'{pos}{int_to_char[new_pos[0]]}{new_pos[1]}{move[2]}'\n",
    "            \n",
    "                if '-' in move_:\n",
    "                    raise Exception()\n",
    "            \n",
    "                if '0' in move_:\n",
    "                    raise Exception()\n",
    "                \n",
    "            except Exception:\n",
    "                move_ = 'XXXX'\n",
    "            moves.append(move_)   \n",
    "        return np.array(moves).reshape(8, 8)\n",
    "    \n",
    "    def init_action_decoder(self):\n",
    "        \"\"\"Initialize the decoder to decode the actions\"\"\"\n",
    "        decoder = []\n",
    "        for key in MOVES.keys():\n",
    "            decoder_ = self.move_board(MOVES[key])\n",
    "            decoder.append(decoder_)\n",
    "#             print(f'Move {key}\\n', new_board, '\\n')\n",
    "    \n",
    "        self.decoder = np.array(decoder)\n",
    "        self.action_size = self.decoder.shape\n",
    "        \n",
    "    def select_action(self, logits):\n",
    "        \"\"\"Decodes the output from the NN to legal actions\"\"\" \n",
    "        decoder_ = np.ndarray.flatten(self.decoder)\n",
    "        logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "        move_logits = [(decoder_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "        move_logits = dict(move_logits)\n",
    "        \n",
    "        legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in self.legal_actions()}\n",
    "        probabilities = list(legal_move_logits.values()) / sum(list(legal_move_logits.values()))\n",
    "\n",
    "        action = random.choices(list(legal_move_logits.keys()), weights = probabilities, k = 1)[0]\n",
    "        return action\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render chess board\"\"\"\n",
    "        chess.svg.board(board = self.board)  \n",
    "\n",
    "env = Chess_Environment()\n",
    "env.board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPLAY BUFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicReplayBuffer:\n",
    "    \"\"\"\n",
    "    Holds the agents episodes in memory\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        self.capacity = params['capacity']\n",
    "        self.memory = deque(maxlen = self.capacity)\n",
    "        self.gamma = params['gamma']\n",
    "\n",
    "    def add(self, episodes):\n",
    "        \"\"\"\n",
    "        Stores the episodes in the replay buffer\n",
    "        \"\"\"\n",
    "        for episode in episodes:\n",
    "            if episode is not None:\n",
    "                self.memory.append(episode)\n",
    "                if len(self.memory) > self.capacity:\n",
    "                    self.memory.popleft() \n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        \"\"\"\n",
    "        Return a sample of size of batch size as an experience tuple.\n",
    "        \"\"\"\n",
    "        if len(self.memory) == 0:\n",
    "            return None\n",
    "        if len(self.memory) >= sample_size:\n",
    "            batch = random.sample(self.memory, k = sample_size)\n",
    "        else:\n",
    "            batch = random.sample(self.memory, k = len(self.memory))\n",
    "#         state, actions, returns_, dones = zip(*batch)\n",
    "\n",
    "#         # Stacks the experiences \n",
    "#         # Get inputs into correct shape\n",
    "#         states = torch.tensor(state).float().to(self.device)\n",
    "# #         actions = torch.stack(actions).to(self.device)\n",
    "#         returns_ = torch.tensor(returns_).float().to(self.device)\n",
    "#         dones = torch.tensor(dones).float().to(self.device)\n",
    "       \n",
    "        # print(\"SAMPLE | STATES\", states.shape, \"ACTIONS\", actions.shape, \"P VECTORS\", portfolio_vectors.shape,\"NEXT STATES\", next_states.shape, \"REWARDS\", rewards.shape, \"DONES\", dones.shape)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Resets the replay buffer\"\"\"\n",
    "        self.memory = deque(maxlen = self.capacity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Running on cuda\n",
      "\n",
      "WARNING OVERRIDDING GPU TO RUN ON CPU\n",
      "CNN_AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(12, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (4): Conv(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (enc_linear): Linear(in_features=512, out_features=300, bias=True)\n",
      "  (dec_linear): Linear(in_features=300, out_features=512, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (4): Conv(\n",
      "      (conv): ConvTranspose2d(32, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "\n",
      "AI Running on cuda\n",
      "\n",
      "WARNING OVERRIDDING GPU TO RUN ON CPU\n",
      "MDN_RNN(\n",
      "  (lstm): LSTM(338, 150, batch_first=True)\n",
      "  (fc1): Linear(in_features=150, out_features=900, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=900, bias=True)\n",
      "  (fc3): Linear(in_features=150, out_features=900, bias=True)\n",
      ") \n",
      "\n",
      "\n",
      "\n",
      "AI Running on cuda\n",
      "\n",
      "WARNING OVERRIDDING GPU TO RUN ON CPU\n",
      "Policy_Controller(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=450, out_features=1200, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): ConvTranspose2d(1200, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): Conv(\n",
      "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): ConvTranspose2d(64, 76, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(76, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "\n",
      "AI Running on cuda\n",
      "\n",
      "WARNING OVERRIDDING GPU TO RUN ON CPU\n",
      "Value_Controller(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=450, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Linear(in_features=20, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, decoder, action_size, vae, mdn, controller, test_mode = False, batch_size = 128, lr = 0.2, legal_multiplier = 3, save_freq = 8, train_times = 4, lambda_ = 0.9, gamma = 0.98):\n",
    "        self.batch_size = batch_size\n",
    "        self.action_size = action_size\n",
    "        self.trained_times = 0\n",
    "        self.train_times = train_times\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.legal_multiplier = legal_multiplier # To increase the value of legal actions and MCTS insight as the target policy\n",
    "        self.decoder = decoder\n",
    "        self.save_freq = save_freq\n",
    "        if vae is not None:\n",
    "            self.vae = CNN_AE(vae, None, 'Latest')\n",
    "            self.vae.eval()\n",
    "        if mdn is not None:\n",
    "            self.mdn = MDN_RNN(mdn, None, 'Latest')\n",
    "            self.mdn.eval()\n",
    "            params = {\n",
    "                'hidden_size' : self.mdn.hidden_size,\n",
    "                'z_size' : self.vae.z_size,\n",
    "                'action_size' : action_size,\n",
    "                'expansion_size': 1200\n",
    "            }\n",
    "\n",
    "            if test_mode:\n",
    "                self.actor = Policy_Controller(controller, None, 'Latest')\n",
    "                self.critic = Value_Controller(controller, None, 'Latest')\n",
    "            else:\n",
    "                self.actor = Policy_Controller(controller, params, False)\n",
    "                self.critic = Value_Controller(controller, params, False)\n",
    "                self.actor.save_model(0)\n",
    "                self.critic.save_model(0)\n",
    "        \n",
    "            self.actor_optimizer = Adam(self.actor.parameters(), lr = lr)\n",
    "            self.critic_optimizer = Adam(self.critic.parameters(), lr = lr)\n",
    "        \n",
    "        self.char_to_int = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8} \n",
    "        self.char_to_int_promo = {'r': 1, 'b': 2, 'q': 3, 'k': 4, 'n': 5, '': 6} \n",
    "        \n",
    "        moves_df = pd.DataFrame(list(zip(self.char_to_int.values(), self.char_to_int.keys())), columns = ['Ids', 'Labels'])\n",
    "        promo_df = pd.DataFrame(list(zip(self.char_to_int_promo.values(), self.char_to_int_promo.keys())), columns = ['Ids', 'Labels'])\n",
    "        self.move_actions_ohe = pd.get_dummies(moves_df['Ids'])\n",
    "        self.promo_actions_ohe = pd.get_dummies(promo_df['Ids'])\n",
    "        \n",
    "        self.criterion_pi = nn.NLLLoss()\n",
    "        self.criterion_v = nn.SmoothL1Loss()\n",
    "    \n",
    "    def select_action(self, logits, legal_actions):\n",
    "        \"\"\"Decodes the output from the NN to legal actions\"\"\" \n",
    "        decoder_ = np.ndarray.flatten(self.decoder)\n",
    "        logits_ = np.ndarray.flatten(logits)\n",
    "\n",
    "        move_logits = [(decoder_[idx].lower(), logits_[idx]) for idx in range(len(logits_))]\n",
    "        move_logits = dict(move_logits)\n",
    "        \n",
    "        legal_move_logits = {legal_action: move_logits[legal_action] for legal_action in legal_actions}\n",
    "        probabilities = list(legal_move_logits.values()) / sum(list(legal_move_logits.values()))\n",
    "        \n",
    "        legal_actions = list(legal_move_logits.keys())\n",
    "        action = random.choices(list(legal_move_logits.keys()), weights = probabilities, k = 1)[0]\n",
    "        \n",
    "        action_probabilities = {legal_actions[idx] : probabilities[idx] for idx in range(len(legal_actions))}\n",
    "        return action, action_probabilities\n",
    "    \n",
    "    def one_hot_encode_action(self, action):\n",
    "        \"\"\"One hot encodes the action\"\"\"\n",
    "        if len(action) == 4:\n",
    "            encoded_action = [\n",
    "                self.move_actions_ohe.loc[\n",
    "                self.char_to_int[action[0]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[1]) - 1].values, \n",
    "                self.move_actions_ohe.loc[self.char_to_int[action[2]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[3]) - 1].values,\n",
    "                self.promo_actions_ohe.loc[self.char_to_int_promo[''] - 1].values\n",
    "            ]\n",
    "        else:\n",
    "            encoded_action = [\n",
    "                self.move_actions_ohe.loc[\n",
    "                self.char_to_int[action[0]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[1]) - 1].values, \n",
    "                self.move_actions_ohe.loc[self.char_to_int[action[2]] - 1].values, \n",
    "                self.move_actions_ohe.loc[int(action[3]) - 1].values,\n",
    "                self.promo_actions_ohe.loc[self.char_to_int_promo[action[4]] - 1].values\n",
    "            ]\n",
    "        ohe_action = np.concatenate(encoded_action)\n",
    "        return ohe_action\n",
    "    \n",
    "    def reset(self, batch_size):\n",
    "        \"\"\"Resets the MDNs hidden state\"\"\"\n",
    "        self.hidden = self.mdn.init_hidden(batch_size)\n",
    "        self.hidden = (self.hidden[0].detach().to('cpu'), self.hidden[1].detach().to('cpu'))\n",
    "        \n",
    "    def act(self, state, legal_actions):\n",
    "        \"\"\"Gets an action from the agent\"\"\"\n",
    "        state = torch.tensor(state).float().unsqueeze(0)\n",
    "        z = self.vae.encode(state)\n",
    "    \n",
    "        zh = torch.cat((z.to('cpu'), self.hidden[0].squeeze(0).to('cpu')), dim = 1)\n",
    "        if zh.shape[0] > 1:\n",
    "            raise Exception('Batch size > 1 not handled')\n",
    "            \n",
    "        logits = self.actor(zh)\n",
    "        values = self.critic(zh)\n",
    "        logits, value = logits.detach().squeeze(0).cpu().numpy(), values.detach().squeeze(0).cpu().numpy()[0]\n",
    "        action, action_probabilities = self.select_action(logits, legal_actions)\n",
    "        \n",
    "        return action_probabilities, value, z\n",
    "    \n",
    "    def encode_z(self, state):\n",
    "        \"\"\"Encodes the state using the autoencoder\"\"\"\n",
    "        state = torch.tensor(state).float().unsqueeze(0)\n",
    "        return self.vae.encode(state)\n",
    "    \n",
    "    def train(self, episodes):\n",
    "        \"\"\"Trains the controller from the MCTS experiences\"\"\"\n",
    "        all_zh = []\n",
    "        all_returns = []\n",
    "        all_mcts_action_probs = []\n",
    "#         batches = [experiences[i : i + self.batch_size] for i in range(0, len(experiences), self.batch_size)]\n",
    "        for idx, episode in enumerate(episodes):\n",
    "            zh, mcts_action_probs, rewards, masks = zip(*episode)\n",
    "            \n",
    "            zh_ = torch.tensor(zh).float().unsqueeze(1)\n",
    "            values = self.critic(zh_.to(self.critic.device))\n",
    "            \n",
    "            # Calculate GAE\n",
    "            returns = []\n",
    "            values_ = list(values.view(-1).detach().cpu().numpy())\n",
    "            gae = 0\n",
    "\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                if i == len(rewards) - 1:\n",
    "                    delta = rewards[i] - values[i]\n",
    "                else:\n",
    "                    delta = rewards[i] + self.gamma * values[i + 1] * masks[i] - values[i]\n",
    "                gae = delta + self.gamma * self.lambda_ * masks[i] * gae\n",
    "                returns.insert(0, gae + values[i])\n",
    "            all_returns.extend(returns)\n",
    "            all_zh.extend(list(zh))\n",
    "            all_mcts_action_probs.extend([x for x in mcts_action_probs])\n",
    "            \n",
    "#         all_returns = [item for sublist in all_returns for item in sublist]\n",
    "#         all_zh = [item for sublist in all_zh for item in sublist]\n",
    "#         all_mcts_action_probs = [item for sublist in all_mcts_action_probs for item in sublist]\n",
    "        \n",
    "        # Trains on random experiences to decoralate the information\n",
    "        idxs = random.choices(range(len(all_zh)), k = self.batch_size)\n",
    "        mcts_action_probs = [all_mcts_action_probs[idx] for idx in idxs]\n",
    "        returns = torch.stack([torch.tensor(all_returns[idx]).float() for idx in idxs]).to(self.critic.device)\n",
    "        actor_zh = torch.stack([torch.tensor(all_zh[idx]).float() for idx in idxs]).to(self.actor.device)\n",
    "        critic_zh = actor_zh.clone().to(self.critic.device)\n",
    "        \n",
    "        logits = self.actor(actor_zh)\n",
    "        values = self.critic(critic_zh)\n",
    "                              \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "#             print(zh.is_cuda, type(zh))\n",
    "        improvement_estimate = self.improvement_estimate(logits, mcts_action_probs)\n",
    "        epsilon = 1e-7\n",
    "#             print(logits.shape, torch.log(logits + epsilon))\n",
    "#             print(improvement_estimate.shape, improvement_estimate)\n",
    "            \n",
    "        actor_losses = -improvement_estimate * torch.log(logits + epsilon)\n",
    "        critic_losses = (returns.view(-1) - values.view(-1)).pow(2)\n",
    "            \n",
    "#             print(actor_losses)\n",
    "#             print(\"IMP EST\", improvement_estimate.shape, \"LOGITS\", logits.shape, \"ACTOR LOSS\", actor_losses.shape)\n",
    "#             print(\"RETURNS\", returns.view(-1).shape, \"VALUES\", values.view(-1).shape, \"CRITIC LOSS\", critic_losses.shape)\n",
    "\n",
    "            \n",
    "#             for idx in range(logits.shape[0]):\n",
    "#                 advantage = returns[idx] - values[idx]\n",
    "#                 # converts logits to action probabilities\n",
    "# #                 print(values[idx].squeeze(0).shape, returns[idx].shape)\n",
    "# #                 print(values[idx].squeeze(0), returns[idx])\n",
    "#                 policy_losses.append(-F.softmax(logits[idx], dim = 0) * advantage)\n",
    "#                 value_losses.append(F.smooth_l1_loss(values[idx].squeeze(0), returns[idx]))\n",
    "            \n",
    "            # sum up all the values of policy_losses and value_losses\n",
    "        actor_loss = actor_losses.sum()\n",
    "#             critic_loss = critic_losses.sum()\n",
    "        critic_loss = critic_losses.sum()\n",
    "            \n",
    "#             writer.add_scalar('policy_loss', policy_loss, writer.games)\n",
    "#             writer.add_scalar('value_loss_loss', value_loss, writer.games)\n",
    "#             loss = policy_loss + value_loss\n",
    "            \n",
    "        print(\"ACTOR LOSS\", actor_loss.item(), \"| CRITIC LOSS\", critic_loss.item())\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        if self.trained_times % self.save_freq == 0:\n",
    "            print(\"SAVING\")\n",
    "            self.actor.save_model(self.trained_times)\n",
    "            self.critic.save_model(self.trained_times)\n",
    "        self.trained_times += 1\n",
    "        \n",
    "    def load_hidden(self, hidden):\n",
    "        \"\"\"Loads the hidden state to the MDN RNN\"\"\"\n",
    "        self.hidden = deepcopy(hidden)\n",
    "        \n",
    "    def get_hidden(self):\n",
    "        \"\"\"Gets the hidden state from the MDN RNN\"\"\"\n",
    "        return deepcopy(self.hidden)\n",
    "    \n",
    "    def random_controller(self): \n",
    "        \"\"\"Updates the controller from a randomly selected save file\"\"\"\n",
    "        path = f'checkpoints_/{self.actor.type}/{self.actor.name}/'\n",
    "        list_of_files = [x for x in os.listdir(path) if '.pth' in x]\n",
    "        file_name = random.choice(list_of_files)\n",
    "        print(f'Loading Random Controllers {file_name}')\n",
    "        \n",
    "        self.actor.load_model(file_name)\n",
    "        self.critic.load_model(file_name)\n",
    "        \n",
    "    def step(self, z, action):\n",
    "        \"\"\"Updates the hidden state of the MDN-RNN\"\"\"\n",
    "        z.squeeze_(-1)\n",
    "        ohe_action = self.one_hot_encode_action(action)\n",
    "        za = torch.cat((z.to('cpu'), torch.tensor(ohe_action).float().to('cpu').unsqueeze(0)), dim = 1)\n",
    "        _, self.hidden = self.mdn(za.unsqueeze(1), self.hidden)\n",
    "        self.hidden = (self.hidden[0].detach().to('cpu'), self.hidden[1].detach().to('cpu'))\n",
    "        \n",
    "                \n",
    "    def improvement_estimate(self, logits, legal_move_logits):\n",
    "        \"\"\"Create an improvement estimate by layering the insights from MCTS\"\"\"\n",
    "        new_boards__ = [[x.lower() for x in list(np.ndarray.flatten(self.decoder))] for _ in range(logits.shape[0])]\n",
    "        improve_est = []\n",
    "        for idx in range(logits.shape[0]):\n",
    "            new_boards___ = new_boards__[idx]\n",
    "            improve_est__ = logits[idx].clone().view(-1)\n",
    "            improve_est__.data.fill_(0)\n",
    "            legal_move_logits__ = {k.lower(): v * self.legal_multiplier for k, v in legal_move_logits[idx].items()}\n",
    "            for move, legal_move_logit in legal_move_logits__.items():\n",
    "                index = new_boards___.index(move)\n",
    "                improve_est__[index] += legal_move_logit\n",
    "            improve_est.append(improve_est__.reshape(76, 8, 8))\n",
    "        return torch.stack(improve_est)\n",
    "    \n",
    "agent = Agent(env.decoder, env.action_size, 'Test', 'Test', 'Test', test_mode = False)\n",
    "agent.reset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING COACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Random Controllers E88.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SanujaPC\\AppData\\Local\\conda\\conda\\envs\\trader\\lib\\site-packages\\ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 3 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 4 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 5 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 6 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 7 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 8 White Won\n",
      "Game 9 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 10 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 11 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 12 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E160.pth\n",
      "Game 4 White Won\n",
      "Game 5 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 6 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 7 White Loss\n",
      "Game 8 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 9 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 10 White Loss\n",
      "Game 11 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 12 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 13 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E128.pth\n",
      "Game 5 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 6 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 7 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 8 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 9 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 10 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 11 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 12 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 13 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 14 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E232.pth\n",
      "Game 6 White Won\n",
      "Game 7 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 8 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 9 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 10 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 11 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 12 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 13 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 14 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 15 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E128.pth\n",
      "Game 7 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 8 White Loss\n",
      "Game 9 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 10 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 11 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 12 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 13 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 14 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 15 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 16 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "EPISODES 1  5\n",
      "EPISODES 2 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SanujaPC\\AppData\\Local\\conda\\conda\\envs\\trader\\lib\\site-packages\\ipykernel_launcher.py:150: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR LOSS 2865.5048828125 | CRITIC LOSS 175.30995178222656\n",
      "SAVING\n",
      "ACTOR LOSS 5327.7236328125 | CRITIC LOSS 3457196.5\n",
      "ACTOR LOSS 5831.173828125 | CRITIC LOSS 57.04170227050781\n",
      "ACTOR LOSS 5950.3232421875 | CRITIC LOSS 411.44354248046875\n",
      "ACTOR LOSS 5849.05859375 | CRITIC LOSS 328.6885070800781\n",
      "ACTOR LOSS 5968.5556640625 | CRITIC LOSS 304.3631896972656\n",
      "ACTOR LOSS 5951.681640625 | CRITIC LOSS 253.7970428466797\n",
      "ACTOR LOSS 5892.7451171875 | CRITIC LOSS 178.30711364746094\n",
      "ACTOR LOSS 6014.52978515625 | CRITIC LOSS 108.84473419189453\n",
      "SAVING\n",
      "ACTOR LOSS 6021.02099609375 | CRITIC LOSS 83.01626586914062\n",
      "Loading Random Controllers E32.pth\n",
      "Game 53 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 54 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 55 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 56 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 57 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 58 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 59 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 60 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 61 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 62 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E136.pth\n",
      "Game 54 White Won\n",
      "Game 55 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 56 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 57 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 58 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 59 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 60 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 61 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 62 White Won\n",
      "Game 63 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E264.pth\n",
      "Game 55 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 56 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 57 White Won\n",
      "Game 58 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 59 White Loss\n",
      "Game 60 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 61 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 62 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 63 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 64 White Won\n",
      "Loading Random Controllers E224.pth\n",
      "Game 56 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 57 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 58 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 59 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 60 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 61 White Loss\n",
      "Game 62 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 63 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 64 White Loss\n",
      "Game 65 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E176.pth\n",
      "Game 57 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 58 White Won\n",
      "Game 59 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 60 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 61 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 62 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 63 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 64 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 65 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 66 White Won\n",
      "EPISODES 1  5\n",
      "EPISODES 2 50\n",
      "ACTOR LOSS 5977.89404296875 | CRITIC LOSS 52406.7890625\n",
      "ACTOR LOSS 5934.68115234375 | CRITIC LOSS 22476.345703125\n",
      "ACTOR LOSS 5882.462890625 | CRITIC LOSS 1425.1285400390625\n",
      "ACTOR LOSS 5701.33837890625 | CRITIC LOSS 680.013916015625\n",
      "ACTOR LOSS 5808.0068359375 | CRITIC LOSS 10197.36328125\n",
      "ACTOR LOSS 5930.0068359375 | CRITIC LOSS 8.126484870910645\n",
      "ACTOR LOSS 5761.02880859375 | CRITIC LOSS 8.204874038696289\n",
      "SAVING\n",
      "ACTOR LOSS 5748.2119140625 | CRITIC LOSS 3716.4404296875\n",
      "ACTOR LOSS 5621.009765625 | CRITIC LOSS 114.65042114257812\n",
      "ACTOR LOSS 5516.2587890625 | CRITIC LOSS 28.10965919494629\n",
      "Loading Random Controllers E208.pth\n",
      "Game 103 White Loss\n",
      "Game 104 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 105 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 106 White Won\n",
      "Game 107 White Won\n",
      "Game 108 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 109 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 110 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 111 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 112 White Loss\n",
      "Loading Random Controllers E40.pth\n",
      "Game 104 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 105 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 106 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 107 White Loss\n",
      "Game 108 White Loss\n",
      "Game 109 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 110 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 111 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 112 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 113 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E256.pth\n",
      "Game 105 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 106 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 107 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 108 White Won\n",
      "Game 109 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 110 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 111 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 112 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 113 White Loss\n",
      "Game 114 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E208.pth\n",
      "Game 106 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 107 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 108 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 109 White Won\n",
      "Game 110 White Loss\n",
      "Game 111 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 112 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 113 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 114 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 115 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E104.pth\n",
      "Game 107 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 108 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 109 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 110 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 111 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 112 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 113 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 114 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 115 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 116 White Won\n",
      "EPISODES 1  5\n",
      "EPISODES 2 50\n",
      "ACTOR LOSS 5164.4189453125 | CRITIC LOSS 4275.10595703125\n",
      "ACTOR LOSS 5297.83935546875 | CRITIC LOSS 35.20058822631836\n",
      "ACTOR LOSS 5406.388671875 | CRITIC LOSS 15851.17578125\n",
      "ACTOR LOSS 5143.91357421875 | CRITIC LOSS 17.1292667388916\n",
      "ACTOR LOSS 5435.69091796875 | CRITIC LOSS 40232.1015625\n",
      "SAVING\n",
      "ACTOR LOSS 5406.9541015625 | CRITIC LOSS 44.47748565673828\n",
      "ACTOR LOSS 5442.544921875 | CRITIC LOSS 202.68418884277344\n",
      "ACTOR LOSS 5365.6943359375 | CRITIC LOSS 36.85049819946289\n",
      "ACTOR LOSS 5330.6103515625 | CRITIC LOSS 66.6737060546875\n",
      "ACTOR LOSS 4888.29248046875 | CRITIC LOSS 231132.828125\n",
      "Loading Random Controllers E56.pth\n",
      "Game 153 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 154 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 155 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 156 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 157 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 158 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 159 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 160 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 161 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 162 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E224.pth\n",
      "Game 154 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 155 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 156 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 157 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 158 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 159 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 160 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 161 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 162 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 163 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E40.pth\n",
      "Game 155 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 156 White Loss\n",
      "Game 157 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 158 White Loss\n",
      "Game 159 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 160 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 161 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 162 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 163 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 164 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E80.pth\n",
      "Game 156 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 157 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 158 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 159 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 160 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 161 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 162 White Loss\n",
      "Game 163 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 164 White Won\n",
      "Game 165 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Loading Random Controllers E40.pth\n",
      "Game 157 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 158 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 159 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 160 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 161 White Won\n",
      "Game 162 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 163 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 164 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 165 Draw\n",
      "WARNING DELETING EXPERIENCES\n",
      "Game 166 White Loss\n",
      "EPISODES 1  5\n",
      "EPISODES 2 50\n",
      "ACTOR LOSS 4992.7177734375 | CRITIC LOSS 50.898860931396484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR LOSS 5309.2607421875 | CRITIC LOSS 185.71524047851562\n",
      "ACTOR LOSS 5136.9599609375 | CRITIC LOSS 97.59239196777344\n",
      "SAVING\n",
      "ACTOR LOSS 5245.947265625 | CRITIC LOSS 143.8649444580078\n",
      "ACTOR LOSS 4007.33935546875 | CRITIC LOSS 172.39842224121094\n",
      "ACTOR LOSS 5036.11279296875 | CRITIC LOSS 112135.84375\n",
      "ACTOR LOSS 4978.6220703125 | CRITIC LOSS 6168.53125\n",
      "ACTOR LOSS 4853.4365234375 | CRITIC LOSS 14197.546875\n",
      "ACTOR LOSS 2724.237548828125 | CRITIC LOSS 442.3494873046875\n",
      "ACTOR LOSS 4631.40966796875 | CRITIC LOSS 63.10688400268555\n",
      "Loading Random Controllers E176.pth\n"
     ]
    }
   ],
   "source": [
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning. It uses the functions defined\n",
    "    in Game and NeuralNet. args are specified in main.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, agent, params):\n",
    "        self.iterations = params['iterations']\n",
    "        self.episodes = params['episodes']\n",
    "        self.queue_length = params['queue_length']\n",
    "        self.simulations = params['simulations']\n",
    "        self.train_times = params['train_times']\n",
    "        self.memory_samples = params['memory_samples']\n",
    "        self.env = deepcopy(env)\n",
    "        self.agent = agent\n",
    "        self.trainExamplesHistory = []  # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.games = 0\n",
    "        \n",
    "        params = {\n",
    "            'capacity' : 300,\n",
    "            'gamma' : 0.98\n",
    "        } \n",
    "        \n",
    "        self.memory = EpisodicReplayBuffer(params)\n",
    "        \n",
    " #     @ray.remote   \n",
    "    def execute_iterations(self, agent, game):\n",
    "        \"\"\"Executes several episodes while preserving the MCTS search tree to speed up inference drastically\"\"\"\n",
    "        mcts_white = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = self.simulations)\n",
    "        mcts_black = MCTS(deepcopy(env), deepcopy(agent), mcts_simulations = self.simulations)  \n",
    "        mcts_black.agent.random_controller()\n",
    "        \n",
    "        episodes = []\n",
    "        for idx in range(1, self.iterations + 1):\n",
    "            for idx2 in range(1, self.episodes + 1):\n",
    "                experiences, mcts_white, mcts_black = self.execute_episode(self.agent, game + idx2 + (idx * self.episodes), mcts_white, mcts_black)\n",
    "                episodes.append(experiences)\n",
    "        \n",
    "        return episodes\n",
    "            \n",
    "\n",
    "    def execute_episode(self, agent, game, mcts_white, mcts_black):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard, currPlayer, pi,v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, else -1.\n",
    "        \"\"\"\n",
    "        env = Chess_Environment()\n",
    "        experiences = []\n",
    "        agent.reset(1)\n",
    "\n",
    "       \n",
    "        game_move = 0\n",
    "        \n",
    "        terminal_test = env.terminal_test()\n",
    "        while not terminal_test:\n",
    "            encoded_state = env.encode()\n",
    "            if env.whites_turn: \n",
    "                action_probs, zh = mcts_white.action_probabilities(encoded_state)\n",
    "            else:\n",
    "                action_probs, zh = mcts_black.action_probabilities(encoded_state)\n",
    "\n",
    "            if env.whites_turn:\n",
    "                experiences.append([zh.detach().cpu().numpy(), action_probs, 1, terminal_test]) # mask is 'not done'\n",
    "            action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "            _, _ = env.step(action)\n",
    "    \n",
    "            game_move += 1\n",
    "            terminal_test = env.terminal_test()\n",
    "        \n",
    "        reward = env.result()\n",
    "        if not env.whites_turn and ((reward == 1) or (reward == -1)):\n",
    "            reward = reward * -1\n",
    "        \n",
    "        experiences[-1][-1] = 0 # mask is 'not done'\n",
    "        experiences[-1][-2] = reward\n",
    "        \n",
    "        if reward == -1:\n",
    "            print(f\"Game {game} White Loss\")\n",
    "        elif reward == 1:\n",
    "            print(f\"Game {game} White Won\")\n",
    "        else:\n",
    "            print(f\"Game {game} Draw\")\n",
    "            print(\"WARNING DELETING EXPERIENCES\")\n",
    "            return None, mcts_white, mcts_black\n",
    "        return experiences, mcts_white, mcts_black\n",
    "    \n",
    "#     def learn(self, ray):    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "        \n",
    "#         for i in range(1, self.iterations + 1):\n",
    "        if PARALLEL:\n",
    "            episodes = ray.get([self.execute_iterations.remote(self, self.agent, self.games + idx) for idx in range(self.iterations)])\n",
    "        else:\n",
    "            episodes = [self.execute_iterations(self.agent, self.games + idx) for idx in range(self.iterations)]\n",
    "        \n",
    "        print(\"EPISODES 1\", len(episodes))\n",
    "        episodes = [item for sublist in episodes for item in sublist]\n",
    "        self.games += len(episodes) \n",
    "        print(\"EPISODES 2\", len(episodes))\n",
    "        \n",
    "        \n",
    "        if episodes is not None:\n",
    "            self.memory.add(episodes)\n",
    "        for _ in range(self.train_times):\n",
    "            episodes = self.memory.sample(self.memory_samples)\n",
    "            if episodes is not None:\n",
    "                self.agent.train(episodes)\n",
    "\n",
    "#         # training new network, keeping a copy of the old one\n",
    "#         self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "#         self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "#         pmcts = MCTS(self.game, self.pnet, self.args)\n",
    "\n",
    "#         \n",
    "#         nmcts = MCTS(self.game, self.nnet, self.args)\n",
    "\n",
    "#         log.info('PITTING AGAINST PREVIOUS VERSION')\n",
    "#         arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n",
    "#                           lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n",
    "#         pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n",
    "\n",
    "#         log.info('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "#         if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.args.updateThreshold:\n",
    "#             log.info('REJECTING NEW MODEL')\n",
    "#             self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "#         else:\n",
    "#             log.info('ACCEPTING NEW MODEL')\n",
    "#             self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n",
    "#             self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')\n",
    "        \n",
    "if PARALLEL:\n",
    "    params = {\n",
    "        'iterations': 5,\n",
    "        'episodes': 16,\n",
    "        'queue_length' : 2000,\n",
    "        'simulations' : 200,\n",
    "        'train_times' : 10,\n",
    "        'memory_samples' : 32\n",
    "    }\n",
    "else:\n",
    "    params = {\n",
    "        'iterations': 5,\n",
    "        'episodes': 2,\n",
    "        'queue_length' : 2000,\n",
    "        'simulations' : 2,\n",
    "        'train_times' : 10,\n",
    "        'memory_samples' : 32\n",
    "    }\n",
    "\n",
    "coach = Coach(env, agent, params)\n",
    "for _ in range(1000):\n",
    "    if PARALLEL:\n",
    "        coach.learn(ray)\n",
    "    else:\n",
    "        coach.learn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWindow(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setGeometry(100, 100, 1100, 1100)\n",
    "        self.widgetSvg = QSvgWidget(parent = self)\n",
    "        self.widgetSvg.setGeometry(10, 10, 1080, 1080)\n",
    "\n",
    "\n",
    "# board = chess.Board()\n",
    "# app = QApplication([])\n",
    "# window = MainWindow()\n",
    "# window.show()\n",
    "# app.exec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arena:\n",
    "    \"\"\"An Arena where you can have two players fight\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env        \n",
    "        self.agent = agent\n",
    "        self.root = tk.Tk()\n",
    "        self.root.withdraw()\n",
    "        self.app = QApplication([])\n",
    "        self.window = MainWindow()\n",
    "    \n",
    "    def vs_human(self):\n",
    "        \"\"\"Fight Against the AI\"\"\"\n",
    "        self.agent = Agent(self.env.decoder, self.env.action_size, 'Test', 'Test', 'Test', test_mode = True)\n",
    "        self.mcts_black = MCTS(deepcopy(self.env), deepcopy(self.agent), mcts_simulations = 20)\n",
    "        \n",
    "        self.env.reset()\n",
    "        self.mcts_black.reset()\n",
    "        game_move = 0\n",
    "        while not self.env.terminal_test():\n",
    "            if not self.env.whites_turn:\n",
    "                encoded_state = self.env.encode()\n",
    "                action_probs, zh = self.mcts_black.action_probabilities(encoded_state, temp = 0)\n",
    "                \n",
    "                figureObject, axesObject = plt.subplots()\n",
    "                # Draw the pie chart\n",
    "                axesObject.pie(list(action_probs.values()), labels = list(action_probs.keys()), autopct = '%1.2f', startangle = 90)\n",
    "                # Aspect ratio - equal means pie is a circl\n",
    "                axesObject.axis('equal')\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "                action = random.choices(list(action_probs.keys()), weights = action_probs.values(), k = 1)[0]\n",
    "            else:\n",
    "                legal_actions = self.env.legal_actions() \n",
    "                self.render()\n",
    "                action = simpledialog.askstring(title = \"Move\", prompt = f\"What's your next move?\\n{legal_actions}\")\n",
    "                while action not in legal_actions:\n",
    "                    action = simpledialog.askstring(title = \"Move\", prompt = f\"Invalid Move. What's your next move?\\n{legal_actions}\")\n",
    "                    if action is None:\n",
    "                        break\n",
    "                if action is None:\n",
    "                    break\n",
    "            \n",
    "            _, _ = self.env.step(action)\n",
    "            game_move += 1   \n",
    "        print(env.result())\n",
    "        \n",
    "    def render(self):\n",
    "        \"\"\"Renders the chess board\"\"\"\n",
    "        chessboardSvg = chess.svg.board(self.env.board).encode(\"UTF-8\")\n",
    "        self.window.widgetSvg.load(chessboardSvg)\n",
    "        self.window.show()\n",
    "        self.app.exec()\n",
    "        \n",
    "        \n",
    "arena = Arena(env, agent)\n",
    "arena.vs_human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-trader]",
   "language": "python",
   "name": "conda-env-conda-trader-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
